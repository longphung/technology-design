{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bart Finetuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "  Batch Loss: 18.7929\n",
      "  Batch Loss: 16.6220\n",
      "  Batch Loss: 15.5640\n",
      "  Batch Loss: 15.2382\n",
      "  Batch Loss: 14.5405\n",
      "  Batch Loss: 14.4614\n",
      "  Batch Loss: 14.7551\n",
      "  Batch Loss: 14.0499\n",
      "  Batch Loss: 13.4992\n",
      "  Batch Loss: 13.4783\n",
      "  Batch Loss: 13.0746\n",
      "  Batch Loss: 12.6784\n",
      "  Batch Loss: 12.6247\n",
      "  Batch Loss: 12.3569\n",
      "  Batch Loss: 12.6336\n",
      "  Batch Loss: 12.4861\n",
      "  Batch Loss: 12.2680\n",
      "  Batch Loss: 12.3079\n",
      "  Batch Loss: 12.1260\n",
      "  Batch Loss: 11.9773\n",
      "  Batch Loss: 12.0187\n",
      "  Batch Loss: 11.8305\n",
      "  Batch Loss: 12.0411\n",
      "  Batch Loss: 11.2976\n",
      "  Batch Loss: 11.9437\n",
      "  Batch Loss: 11.5230\n",
      "  Batch Loss: 11.6720\n",
      "  Batch Loss: 11.3411\n",
      "  Batch Loss: 11.3095\n",
      "  Batch Loss: 11.3996\n",
      "  Batch Loss: 11.5677\n",
      "  Batch Loss: 10.9039\n",
      "  Batch Loss: 10.9198\n",
      "  Batch Loss: 11.2832\n",
      "  Batch Loss: 10.7567\n",
      "  Batch Loss: 10.4492\n",
      "  Batch Loss: 10.8263\n",
      "  Batch Loss: 10.7635\n",
      "  Batch Loss: 10.5747\n",
      "  Batch Loss: 10.2325\n",
      "  Batch Loss: 10.6975\n",
      "  Batch Loss: 10.2802\n",
      "  Batch Loss: 10.3521\n",
      "  Batch Loss: 9.9723\n",
      "  Batch Loss: 10.2158\n",
      "  Batch Loss: 9.8789\n",
      "  Batch Loss: 10.3257\n",
      "  Batch Loss: 10.2489\n",
      "  Batch Loss: 10.2714\n",
      "  Batch Loss: 9.7024\n",
      "  Batch Loss: 9.6998\n",
      "  Batch Loss: 9.7860\n",
      "  Batch Loss: 9.8826\n",
      "  Batch Loss: 9.6432\n",
      "  Batch Loss: 9.7719\n",
      "  Batch Loss: 9.7155\n",
      "  Batch Loss: 9.4952\n",
      "  Batch Loss: 9.7106\n",
      "  Batch Loss: 8.9898\n",
      "  Batch Loss: 9.4148\n",
      "  Batch Loss: 9.3437\n",
      "  Batch Loss: 8.9295\n",
      "  Batch Loss: 9.3827\n",
      "  Batch Loss: 9.3014\n",
      "  Batch Loss: 8.6306\n",
      "  Batch Loss: 9.2990\n",
      "  Batch Loss: 8.6234\n",
      "  Batch Loss: 8.7789\n",
      "  Batch Loss: 9.0296\n",
      "  Batch Loss: 8.7098\n",
      "  Batch Loss: 8.6687\n",
      "  Batch Loss: 8.5171\n",
      "  Batch Loss: 8.6450\n",
      "  Batch Loss: 8.5847\n",
      "  Batch Loss: 8.6531\n",
      "  Batch Loss: 8.3036\n",
      "  Batch Loss: 8.3516\n",
      "  Batch Loss: 8.0445\n",
      "  Batch Loss: 8.0674\n",
      "  Batch Loss: 7.9169\n",
      "  Batch Loss: 8.0864\n",
      "  Batch Loss: 8.1022\n",
      "  Batch Loss: 7.8946\n",
      "  Batch Loss: 7.7212\n",
      "  Batch Loss: 7.9518\n",
      "  Batch Loss: 7.9404\n",
      "  Batch Loss: 7.4144\n",
      "  Batch Loss: 7.6916\n",
      "  Batch Loss: 7.6687\n",
      "  Batch Loss: 7.1571\n",
      "  Batch Loss: 7.2843\n",
      "  Batch Loss: 6.8871\n",
      "  Batch Loss: 7.3416\n",
      "  Batch Loss: 7.2001\n",
      "  Batch Loss: 7.0691\n",
      "  Batch Loss: 7.0814\n",
      "  Batch Loss: 7.4096\n",
      "  Batch Loss: 6.8452\n",
      "  Batch Loss: 6.7612\n",
      "  Batch Loss: 6.7743\n",
      "  Batch Loss: 6.7483\n",
      "  Batch Loss: 6.8237\n",
      "  Batch Loss: 6.7675\n",
      "  Batch Loss: 6.6899\n",
      "  Batch Loss: 6.4041\n",
      "  Batch Loss: 6.3533\n",
      "  Batch Loss: 6.3615\n",
      "  Batch Loss: 6.2850\n",
      "  Batch Loss: 6.2942\n",
      "  Batch Loss: 6.1818\n",
      "  Batch Loss: 6.2898\n",
      "  Batch Loss: 6.1933\n",
      "  Batch Loss: 6.3175\n",
      "  Batch Loss: 6.0584\n",
      "  Batch Loss: 6.0101\n",
      "  Batch Loss: 6.1682\n",
      "  Batch Loss: 6.1666\n",
      "  Batch Loss: 5.9619\n",
      "  Batch Loss: 5.9391\n",
      "  Batch Loss: 6.1496\n",
      "  Batch Loss: 5.9984\n",
      "  Batch Loss: 6.0754\n",
      "  Batch Loss: 5.8993\n",
      "  Batch Loss: 5.8349\n",
      "  Batch Loss: 6.0337\n",
      "  Batch Loss: 5.8431\n",
      "  Batch Loss: 6.1435\n",
      "  Batch Loss: 5.8515\n",
      "  Batch Loss: 5.9674\n",
      "  Batch Loss: 5.7843\n",
      "  Batch Loss: 5.7876\n",
      "  Batch Loss: 5.7404\n",
      "  Batch Loss: 5.8054\n",
      "  Batch Loss: 5.6839\n",
      "  Batch Loss: 5.7959\n",
      "  Batch Loss: 5.6146\n",
      "  Batch Loss: 5.5951\n",
      "  Batch Loss: 5.5970\n",
      "  Batch Loss: 5.6743\n",
      "  Batch Loss: 5.6058\n",
      "  Batch Loss: 5.6586\n",
      "  Batch Loss: 5.6359\n",
      "  Batch Loss: 5.5648\n",
      "  Batch Loss: 5.5843\n",
      "  Batch Loss: 5.5009\n",
      "  Batch Loss: 5.7312\n",
      "  Batch Loss: 5.4944\n",
      "  Batch Loss: 5.5022\n",
      "  Batch Loss: 5.3493\n",
      "  Batch Loss: 5.4814\n",
      "  Batch Loss: 5.3996\n",
      "  Batch Loss: 5.4591\n",
      "  Batch Loss: 5.5103\n",
      "  Batch Loss: 5.1623\n",
      "  Batch Loss: 5.4115\n",
      "  Batch Loss: 5.3851\n",
      "  Batch Loss: 5.3463\n",
      "  Batch Loss: 5.3361\n",
      "  Batch Loss: 5.3581\n",
      "  Batch Loss: 5.2355\n",
      "  Batch Loss: 5.0529\n",
      "  Batch Loss: 5.3053\n",
      "  Batch Loss: 5.2405\n",
      "  Batch Loss: 5.2208\n",
      "  Batch Loss: 5.3388\n",
      "  Batch Loss: 5.1303\n",
      "  Batch Loss: 5.1416\n",
      "  Batch Loss: 5.2004\n",
      "  Batch Loss: 5.2175\n",
      "  Batch Loss: 5.1704\n",
      "  Batch Loss: 5.3557\n",
      "  Batch Loss: 5.0548\n",
      "  Batch Loss: 5.2470\n",
      "  Batch Loss: 5.1961\n",
      "  Batch Loss: 5.0554\n",
      "  Batch Loss: 5.1350\n",
      "  Batch Loss: 5.1124\n",
      "  Batch Loss: 5.0743\n",
      "  Batch Loss: 4.8937\n",
      "  Batch Loss: 4.9591\n",
      "  Batch Loss: 5.1099\n",
      "  Batch Loss: 4.9352\n",
      "  Batch Loss: 4.9212\n",
      "  Batch Loss: 4.8438\n",
      "  Batch Loss: 4.8971\n",
      "  Batch Loss: 4.8831\n",
      "  Batch Loss: 4.9747\n",
      "  Batch Loss: 4.9925\n",
      "  Batch Loss: 4.8491\n",
      "  Batch Loss: 4.9003\n",
      "  Batch Loss: 4.8239\n",
      "  Batch Loss: 4.8455\n",
      "  Batch Loss: 4.9205\n",
      "  Batch Loss: 4.9312\n",
      "  Batch Loss: 4.8065\n",
      "  Batch Loss: 4.7791\n",
      "  Batch Loss: 4.8150\n",
      "  Batch Loss: 4.8249\n",
      "  Batch Loss: 4.8381\n",
      "  Batch Loss: 4.7242\n",
      "  Batch Loss: 4.7142\n",
      "  Batch Loss: 4.7768\n",
      "  Batch Loss: 4.6076\n",
      "  Batch Loss: 4.6958\n",
      "  Batch Loss: 4.6984\n",
      "  Batch Loss: 4.6766\n",
      "  Batch Loss: 4.6527\n",
      "  Batch Loss: 4.5202\n",
      "  Batch Loss: 4.6816\n",
      "  Batch Loss: 4.5666\n",
      "  Batch Loss: 4.6174\n",
      "  Batch Loss: 4.6647\n",
      "  Batch Loss: 4.5303\n",
      "  Batch Loss: 4.4957\n",
      "  Batch Loss: 4.5139\n",
      "  Batch Loss: 4.5283\n",
      "  Batch Loss: 4.5190\n",
      "  Batch Loss: 4.5192\n",
      "  Batch Loss: 4.5123\n",
      "  Batch Loss: 4.4861\n",
      "  Batch Loss: 4.4284\n",
      "  Batch Loss: 4.3569\n",
      "  Batch Loss: 4.4192\n",
      "  Batch Loss: 4.4324\n",
      "  Batch Loss: 4.2993\n",
      "  Batch Loss: 4.4037\n",
      "  Batch Loss: 4.3717\n",
      "  Batch Loss: 4.4279\n",
      "  Batch Loss: 4.4009\n",
      "  Batch Loss: 4.3950\n",
      "  Batch Loss: 4.3240\n",
      "  Batch Loss: 4.2957\n",
      "  Batch Loss: 4.3360\n",
      "  Batch Loss: 4.2446\n",
      "  Batch Loss: 4.2071\n",
      "  Batch Loss: 4.1673\n",
      "  Batch Loss: 4.2571\n",
      "  Batch Loss: 4.1955\n",
      "  Batch Loss: 4.2172\n",
      "  Batch Loss: 4.1800\n",
      "  Batch Loss: 4.2011\n",
      "  Batch Loss: 4.1866\n",
      "  Batch Loss: 4.4790\n",
      "  Batch Loss: 4.1459\n",
      "  Batch Loss: 4.1206\n",
      "  Batch Loss: 4.1110\n",
      "  Batch Loss: 4.1411\n",
      "  Batch Loss: 4.0966\n",
      "  Batch Loss: 3.9730\n",
      "  Batch Loss: 4.1344\n",
      "  Batch Loss: 4.0638\n",
      "  Batch Loss: 3.9621\n",
      "  Batch Loss: 3.8920\n",
      "  Batch Loss: 3.9348\n",
      "  Batch Loss: 4.0001\n",
      "  Batch Loss: 3.9885\n",
      "  Batch Loss: 3.9823\n",
      "  Batch Loss: 3.8710\n",
      "  Batch Loss: 3.8065\n",
      "  Batch Loss: 3.8493\n",
      "  Batch Loss: 3.8640\n",
      "  Batch Loss: 3.8853\n",
      "  Batch Loss: 3.7177\n",
      "  Batch Loss: 3.7621\n",
      "  Batch Loss: 3.9310\n",
      "  Batch Loss: 3.8187\n",
      "  Batch Loss: 3.8039\n",
      "  Batch Loss: 3.8016\n",
      "  Batch Loss: 3.7397\n",
      "  Batch Loss: 3.8262\n",
      "  Batch Loss: 3.7623\n",
      "  Batch Loss: 3.7061\n",
      "  Batch Loss: 3.7196\n",
      "  Batch Loss: 3.6976\n",
      "  Batch Loss: 3.6315\n",
      "  Batch Loss: 3.6674\n",
      "  Batch Loss: 3.6262\n",
      "  Batch Loss: 3.6094\n",
      "  Batch Loss: 3.6091\n",
      "  Batch Loss: 3.6631\n",
      "  Batch Loss: 3.6066\n",
      "  Batch Loss: 3.5821\n",
      "  Batch Loss: 3.5701\n",
      "  Batch Loss: 3.7871\n",
      "  Batch Loss: 3.4847\n",
      "  Batch Loss: 3.5041\n",
      "  Batch Loss: 3.5575\n",
      "  Batch Loss: 3.5254\n",
      "  Batch Loss: 3.4257\n",
      "  Batch Loss: 3.4971\n",
      "  Batch Loss: 3.4485\n",
      "  Batch Loss: 3.4268\n",
      "  Batch Loss: 3.4299\n",
      "  Batch Loss: 3.4681\n",
      "  Batch Loss: 3.3484\n",
      "  Batch Loss: 3.3776\n",
      "  Batch Loss: 3.3978\n",
      "  Batch Loss: 3.3595\n",
      "  Batch Loss: 3.3137\n",
      "  Batch Loss: 3.3348\n",
      "  Batch Loss: 3.3291\n",
      "  Batch Loss: 3.3421\n",
      "  Batch Loss: 3.2860\n",
      "  Batch Loss: 3.2665\n",
      "  Batch Loss: 3.2299\n",
      "  Batch Loss: 3.1981\n",
      "  Batch Loss: 3.2526\n",
      "  Batch Loss: 3.1826\n",
      "  Batch Loss: 3.2096\n",
      "  Batch Loss: 3.1704\n",
      "  Batch Loss: 3.1419\n",
      "  Batch Loss: 3.0775\n",
      "  Batch Loss: 3.1527\n",
      "  Batch Loss: 3.1251\n",
      "  Batch Loss: 3.0939\n",
      "  Batch Loss: 3.0908\n",
      "  Batch Loss: 3.0518\n",
      "  Batch Loss: 3.0817\n",
      "  Batch Loss: 3.0659\n",
      "  Batch Loss: 2.9473\n",
      "  Batch Loss: 3.0565\n",
      "  Batch Loss: 2.9938\n",
      "  Batch Loss: 2.9574\n",
      "  Batch Loss: 2.9304\n",
      "  Batch Loss: 3.0411\n",
      "  Batch Loss: 2.9627\n",
      "  Batch Loss: 2.9192\n",
      "  Batch Loss: 2.9673\n",
      "  Batch Loss: 3.2177\n",
      "  Batch Loss: 2.8958\n",
      "  Batch Loss: 2.9935\n",
      "  Batch Loss: 2.7577\n",
      "  Batch Loss: 2.9528\n",
      "  Batch Loss: 2.8779\n",
      "  Batch Loss: 2.8939\n",
      "  Batch Loss: 2.8938\n",
      "  Batch Loss: 2.7918\n",
      "  Batch Loss: 2.7270\n",
      "  Batch Loss: 2.7978\n",
      "  Batch Loss: 2.7924\n",
      "  Batch Loss: 2.9316\n",
      "  Batch Loss: 2.7182\n",
      "  Batch Loss: 2.7288\n",
      "  Batch Loss: 2.9393\n",
      "  Batch Loss: 2.7848\n",
      "  Batch Loss: 2.6194\n",
      "  Batch Loss: 2.5927\n",
      "  Batch Loss: 2.5893\n",
      "  Batch Loss: 2.6562\n",
      "  Batch Loss: 2.5684\n",
      "  Batch Loss: 2.8068\n",
      "  Batch Loss: 2.5756\n",
      "  Batch Loss: 2.7652\n",
      "  Batch Loss: 2.6363\n",
      "  Batch Loss: 2.4650\n",
      "  Batch Loss: 2.4652\n",
      "  Batch Loss: 2.5717\n",
      "  Batch Loss: 2.4198\n",
      "  Batch Loss: 2.4451\n",
      "  Batch Loss: 2.4487\n",
      "  Batch Loss: 2.5776\n",
      "  Batch Loss: 2.4352\n",
      "  Batch Loss: 2.4887\n",
      "  Batch Loss: 2.3590\n",
      "  Batch Loss: 2.4389\n",
      "  Batch Loss: 2.3517\n",
      "  Batch Loss: 2.3530\n",
      "  Batch Loss: 2.2862\n",
      "  Batch Loss: 2.2877\n",
      "  Batch Loss: 2.3339\n",
      "  Batch Loss: 2.3307\n",
      "  Batch Loss: 2.2734\n",
      "  Batch Loss: 2.2809\n",
      "  Batch Loss: 2.2895\n",
      "  Batch Loss: 2.4182\n",
      "  Batch Loss: 2.2877\n",
      "  Batch Loss: 2.2861\n",
      "  Batch Loss: 2.2353\n",
      "  Batch Loss: 2.1655\n",
      "  Batch Loss: 2.2384\n",
      "  Batch Loss: 2.2197\n",
      "  Batch Loss: 2.1895\n",
      "  Batch Loss: 2.1489\n",
      "  Batch Loss: 2.1057\n",
      "  Batch Loss: 2.0522\n",
      "  Batch Loss: 2.1419\n",
      "  Batch Loss: 2.0433\n",
      "  Batch Loss: 2.1637\n",
      "  Batch Loss: 2.1008\n",
      "  Batch Loss: 2.0109\n",
      "  Batch Loss: 2.1060\n",
      "  Batch Loss: 2.0239\n",
      "  Batch Loss: 2.1805\n",
      "  Batch Loss: 2.0392\n",
      "  Batch Loss: 2.0100\n",
      "  Batch Loss: 2.0215\n",
      "  Batch Loss: 1.8999\n",
      "  Batch Loss: 2.0848\n",
      "  Batch Loss: 1.9407\n",
      "  Batch Loss: 2.0284\n",
      "  Batch Loss: 1.8188\n",
      "  Batch Loss: 1.8510\n",
      "  Batch Loss: 2.0588\n",
      "  Batch Loss: 1.9020\n",
      "  Batch Loss: 2.0229\n",
      "  Batch Loss: 2.0330\n",
      "  Batch Loss: 1.7502\n",
      "  Batch Loss: 2.1899\n",
      "  Batch Loss: 1.8390\n",
      "  Batch Loss: 2.0053\n",
      "  Batch Loss: 1.8732\n",
      "  Batch Loss: 1.8782\n",
      "  Batch Loss: 1.7563\n",
      "  Batch Loss: 1.7142\n",
      "  Batch Loss: 1.8161\n",
      "  Batch Loss: 1.7084\n",
      "  Batch Loss: 1.7943\n",
      "  Batch Loss: 1.6722\n",
      "  Batch Loss: 1.8511\n",
      "  Batch Loss: 1.7773\n",
      "  Batch Loss: 1.6674\n",
      "  Batch Loss: 1.7515\n",
      "  Batch Loss: 1.7694\n",
      "  Batch Loss: 1.6033\n",
      "  Batch Loss: 1.7129\n",
      "  Batch Loss: 1.7751\n",
      "  Batch Loss: 1.4714\n",
      "  Batch Loss: 1.6243\n",
      "  Batch Loss: 1.4997\n",
      "  Batch Loss: 1.7727\n",
      "  Batch Loss: 1.5112\n",
      "  Batch Loss: 1.6037\n",
      "  Batch Loss: 1.5194\n",
      "  Batch Loss: 1.5029\n",
      "  Batch Loss: 1.4411\n",
      "  Batch Loss: 1.4507\n",
      "  Batch Loss: 1.4450\n",
      "  Batch Loss: 1.5081\n",
      "Average Loss: 5.2621\n",
      "Epoch: 2\n",
      "  Batch Loss: 1.4489\n",
      "  Batch Loss: 1.5245\n",
      "  Batch Loss: 1.4891\n",
      "  Batch Loss: 1.6368\n",
      "  Batch Loss: 1.4292\n",
      "  Batch Loss: 1.3095\n",
      "  Batch Loss: 1.5662\n",
      "  Batch Loss: 1.3747\n",
      "  Batch Loss: 1.4404\n",
      "  Batch Loss: 1.4264\n",
      "  Batch Loss: 1.4385\n",
      "  Batch Loss: 1.7631\n",
      "  Batch Loss: 1.1786\n",
      "  Batch Loss: 1.3139\n",
      "  Batch Loss: 1.4275\n",
      "  Batch Loss: 1.2278\n",
      "  Batch Loss: 1.2683\n",
      "  Batch Loss: 1.2809\n",
      "  Batch Loss: 1.3696\n",
      "  Batch Loss: 1.2698\n",
      "  Batch Loss: 1.3410\n",
      "  Batch Loss: 1.4103\n",
      "  Batch Loss: 1.1727\n",
      "  Batch Loss: 1.2973\n",
      "  Batch Loss: 1.4419\n",
      "  Batch Loss: 1.3264\n",
      "  Batch Loss: 1.3129\n",
      "  Batch Loss: 1.2967\n",
      "  Batch Loss: 1.2862\n",
      "  Batch Loss: 1.0689\n",
      "  Batch Loss: 1.1624\n",
      "  Batch Loss: 1.1978\n",
      "  Batch Loss: 1.7160\n",
      "  Batch Loss: 1.1477\n",
      "  Batch Loss: 1.2609\n",
      "  Batch Loss: 1.2024\n",
      "  Batch Loss: 0.9992\n",
      "  Batch Loss: 1.0903\n",
      "  Batch Loss: 1.1317\n",
      "  Batch Loss: 1.0188\n",
      "  Batch Loss: 1.2922\n",
      "  Batch Loss: 1.3382\n",
      "  Batch Loss: 1.1127\n",
      "  Batch Loss: 1.0902\n",
      "  Batch Loss: 1.2361\n",
      "  Batch Loss: 0.9628\n",
      "  Batch Loss: 0.9430\n",
      "  Batch Loss: 1.2241\n",
      "  Batch Loss: 1.1671\n",
      "  Batch Loss: 1.0585\n",
      "  Batch Loss: 1.0354\n",
      "  Batch Loss: 1.0798\n",
      "  Batch Loss: 0.8902\n",
      "  Batch Loss: 1.0800\n",
      "  Batch Loss: 1.2783\n",
      "  Batch Loss: 0.9627\n",
      "  Batch Loss: 0.9265\n",
      "  Batch Loss: 0.8405\n",
      "  Batch Loss: 1.1331\n",
      "  Batch Loss: 0.8938\n",
      "  Batch Loss: 0.8937\n",
      "  Batch Loss: 0.9468\n",
      "  Batch Loss: 1.0662\n",
      "  Batch Loss: 0.9245\n",
      "  Batch Loss: 0.8719\n",
      "  Batch Loss: 0.9077\n",
      "  Batch Loss: 0.8891\n",
      "  Batch Loss: 0.8103\n",
      "  Batch Loss: 0.9309\n",
      "  Batch Loss: 0.9307\n",
      "  Batch Loss: 1.0638\n",
      "  Batch Loss: 0.8973\n",
      "  Batch Loss: 1.0705\n",
      "  Batch Loss: 0.8768\n",
      "  Batch Loss: 0.9533\n",
      "  Batch Loss: 0.9103\n",
      "  Batch Loss: 0.7536\n",
      "  Batch Loss: 0.8148\n",
      "  Batch Loss: 0.8097\n",
      "  Batch Loss: 1.0072\n",
      "  Batch Loss: 1.2289\n",
      "  Batch Loss: 0.9867\n",
      "  Batch Loss: 0.7534\n",
      "  Batch Loss: 0.8683\n",
      "  Batch Loss: 0.9185\n",
      "  Batch Loss: 1.0368\n",
      "  Batch Loss: 0.7780\n",
      "  Batch Loss: 0.9306\n",
      "  Batch Loss: 0.9389\n",
      "  Batch Loss: 0.7906\n",
      "  Batch Loss: 0.9562\n",
      "  Batch Loss: 0.8903\n",
      "  Batch Loss: 0.7523\n",
      "  Batch Loss: 1.0389\n",
      "  Batch Loss: 0.9425\n",
      "  Batch Loss: 0.6669\n",
      "  Batch Loss: 0.8714\n",
      "  Batch Loss: 0.8629\n",
      "  Batch Loss: 0.6805\n",
      "  Batch Loss: 0.7861\n",
      "  Batch Loss: 0.9159\n",
      "  Batch Loss: 0.7813\n",
      "  Batch Loss: 0.8136\n",
      "  Batch Loss: 0.7478\n",
      "  Batch Loss: 1.0409\n",
      "  Batch Loss: 0.6612\n",
      "  Batch Loss: 0.9422\n",
      "  Batch Loss: 0.8316\n",
      "  Batch Loss: 0.6929\n",
      "  Batch Loss: 0.7008\n",
      "  Batch Loss: 0.6535\n",
      "  Batch Loss: 0.6125\n",
      "  Batch Loss: 0.9177\n",
      "  Batch Loss: 0.6862\n",
      "  Batch Loss: 0.7826\n",
      "  Batch Loss: 0.9342\n",
      "  Batch Loss: 0.8540\n",
      "  Batch Loss: 1.0258\n",
      "  Batch Loss: 0.6774\n",
      "  Batch Loss: 0.6091\n",
      "  Batch Loss: 0.7897\n",
      "  Batch Loss: 0.6266\n",
      "  Batch Loss: 0.9710\n",
      "  Batch Loss: 0.7680\n",
      "  Batch Loss: 0.6055\n",
      "  Batch Loss: 0.5977\n",
      "  Batch Loss: 0.5815\n",
      "  Batch Loss: 0.7140\n",
      "  Batch Loss: 0.8225\n",
      "  Batch Loss: 0.7380\n",
      "  Batch Loss: 0.7233\n",
      "  Batch Loss: 0.7731\n",
      "  Batch Loss: 0.7575\n",
      "  Batch Loss: 0.8375\n",
      "  Batch Loss: 0.7844\n",
      "  Batch Loss: 0.5761\n",
      "  Batch Loss: 0.6959\n",
      "  Batch Loss: 0.6323\n",
      "  Batch Loss: 0.5515\n",
      "  Batch Loss: 0.7155\n",
      "  Batch Loss: 0.7309\n",
      "  Batch Loss: 0.5404\n",
      "  Batch Loss: 0.7289\n",
      "  Batch Loss: 0.6397\n",
      "  Batch Loss: 0.4960\n",
      "  Batch Loss: 0.6366\n",
      "  Batch Loss: 0.6949\n",
      "  Batch Loss: 0.5946\n",
      "  Batch Loss: 0.5029\n",
      "  Batch Loss: 0.6432\n",
      "  Batch Loss: 0.4235\n",
      "  Batch Loss: 0.7178\n",
      "  Batch Loss: 0.6804\n",
      "  Batch Loss: 0.7682\n",
      "  Batch Loss: 0.4151\n",
      "  Batch Loss: 0.7501\n",
      "  Batch Loss: 0.7327\n",
      "  Batch Loss: 0.6606\n",
      "  Batch Loss: 0.9220\n",
      "  Batch Loss: 0.8914\n",
      "  Batch Loss: 0.6084\n",
      "  Batch Loss: 0.4325\n",
      "  Batch Loss: 0.5467\n",
      "  Batch Loss: 0.5828\n",
      "  Batch Loss: 0.6969\n",
      "  Batch Loss: 0.8290\n",
      "  Batch Loss: 0.7315\n",
      "  Batch Loss: 0.5901\n",
      "  Batch Loss: 0.6247\n",
      "  Batch Loss: 0.5452\n",
      "  Batch Loss: 0.4710\n",
      "  Batch Loss: 0.7066\n",
      "  Batch Loss: 0.6396\n",
      "  Batch Loss: 0.7130\n",
      "  Batch Loss: 0.5810\n",
      "  Batch Loss: 0.6828\n",
      "  Batch Loss: 0.7339\n",
      "  Batch Loss: 0.6689\n",
      "  Batch Loss: 0.7147\n",
      "  Batch Loss: 0.5610\n",
      "  Batch Loss: 0.6146\n",
      "  Batch Loss: 0.4606\n",
      "  Batch Loss: 0.4449\n",
      "  Batch Loss: 0.9029\n",
      "  Batch Loss: 0.5515\n",
      "  Batch Loss: 0.7140\n",
      "  Batch Loss: 0.4156\n",
      "  Batch Loss: 0.6699\n",
      "  Batch Loss: 0.5410\n",
      "  Batch Loss: 0.5444\n",
      "  Batch Loss: 0.7097\n",
      "  Batch Loss: 0.5005\n",
      "  Batch Loss: 0.5210\n",
      "  Batch Loss: 0.6024\n",
      "  Batch Loss: 0.5908\n",
      "  Batch Loss: 0.4497\n",
      "  Batch Loss: 0.5700\n",
      "  Batch Loss: 0.8669\n",
      "  Batch Loss: 0.7652\n",
      "  Batch Loss: 0.5778\n",
      "  Batch Loss: 0.5388\n",
      "  Batch Loss: 0.7679\n",
      "  Batch Loss: 0.6828\n",
      "  Batch Loss: 0.5975\n",
      "  Batch Loss: 0.5402\n",
      "  Batch Loss: 1.1158\n",
      "  Batch Loss: 0.6665\n",
      "  Batch Loss: 0.5731\n",
      "  Batch Loss: 0.4619\n",
      "  Batch Loss: 0.5329\n",
      "  Batch Loss: 0.5553\n",
      "  Batch Loss: 0.6799\n",
      "  Batch Loss: 0.7550\n",
      "  Batch Loss: 0.7714\n",
      "  Batch Loss: 0.5943\n",
      "  Batch Loss: 0.4736\n",
      "  Batch Loss: 0.5424\n",
      "  Batch Loss: 0.8534\n",
      "  Batch Loss: 0.6454\n",
      "  Batch Loss: 0.4643\n",
      "  Batch Loss: 0.6490\n",
      "  Batch Loss: 0.6910\n",
      "  Batch Loss: 0.6807\n",
      "  Batch Loss: 0.8375\n",
      "  Batch Loss: 0.5249\n",
      "  Batch Loss: 0.5809\n",
      "  Batch Loss: 0.5033\n",
      "  Batch Loss: 0.6886\n",
      "  Batch Loss: 0.5346\n",
      "  Batch Loss: 0.5323\n",
      "  Batch Loss: 0.6413\n",
      "  Batch Loss: 1.0939\n",
      "  Batch Loss: 0.6115\n",
      "  Batch Loss: 0.7224\n",
      "  Batch Loss: 1.0128\n",
      "  Batch Loss: 0.4286\n",
      "  Batch Loss: 0.7214\n",
      "  Batch Loss: 0.4523\n",
      "  Batch Loss: 0.5291\n",
      "  Batch Loss: 0.5136\n",
      "  Batch Loss: 0.5504\n",
      "  Batch Loss: 0.5818\n",
      "  Batch Loss: 0.5549\n",
      "  Batch Loss: 0.5143\n",
      "  Batch Loss: 0.3794\n",
      "  Batch Loss: 0.6787\n",
      "  Batch Loss: 0.5172\n",
      "  Batch Loss: 0.5902\n",
      "  Batch Loss: 0.6290\n",
      "  Batch Loss: 0.5059\n",
      "  Batch Loss: 0.5534\n",
      "  Batch Loss: 0.6458\n",
      "  Batch Loss: 0.5237\n",
      "  Batch Loss: 0.6966\n",
      "  Batch Loss: 0.6444\n",
      "  Batch Loss: 0.5495\n",
      "  Batch Loss: 0.5541\n",
      "  Batch Loss: 0.4507\n",
      "  Batch Loss: 0.6109\n",
      "  Batch Loss: 0.5174\n",
      "  Batch Loss: 0.5352\n",
      "  Batch Loss: 0.3980\n",
      "  Batch Loss: 0.4897\n",
      "  Batch Loss: 0.6432\n",
      "  Batch Loss: 0.4531\n",
      "  Batch Loss: 0.6684\n",
      "  Batch Loss: 0.8866\n",
      "  Batch Loss: 0.4917\n",
      "  Batch Loss: 0.5217\n",
      "  Batch Loss: 0.4201\n",
      "  Batch Loss: 0.5748\n",
      "  Batch Loss: 0.4077\n",
      "  Batch Loss: 0.4065\n",
      "  Batch Loss: 0.5946\n",
      "  Batch Loss: 0.5592\n",
      "  Batch Loss: 0.5856\n",
      "  Batch Loss: 0.4909\n",
      "  Batch Loss: 0.3942\n",
      "  Batch Loss: 0.7665\n",
      "  Batch Loss: 0.4902\n",
      "  Batch Loss: 0.4831\n",
      "  Batch Loss: 0.5511\n",
      "  Batch Loss: 0.4038\n",
      "  Batch Loss: 0.6937\n",
      "  Batch Loss: 0.5929\n",
      "  Batch Loss: 0.4545\n",
      "  Batch Loss: 0.4891\n",
      "  Batch Loss: 0.6452\n",
      "  Batch Loss: 0.4806\n",
      "  Batch Loss: 0.3988\n",
      "  Batch Loss: 0.5154\n",
      "  Batch Loss: 0.4207\n",
      "  Batch Loss: 0.4633\n",
      "  Batch Loss: 0.5447\n",
      "  Batch Loss: 0.6009\n",
      "  Batch Loss: 0.5742\n",
      "  Batch Loss: 0.6118\n",
      "  Batch Loss: 0.6396\n",
      "  Batch Loss: 0.2958\n",
      "  Batch Loss: 0.4813\n",
      "  Batch Loss: 0.4166\n",
      "  Batch Loss: 0.4967\n",
      "  Batch Loss: 0.4030\n",
      "  Batch Loss: 0.4499\n",
      "  Batch Loss: 0.5168\n",
      "  Batch Loss: 0.4329\n",
      "  Batch Loss: 0.6744\n",
      "  Batch Loss: 0.5836\n",
      "  Batch Loss: 0.4445\n",
      "  Batch Loss: 0.6348\n",
      "  Batch Loss: 0.5222\n",
      "  Batch Loss: 0.3495\n",
      "  Batch Loss: 0.5422\n",
      "  Batch Loss: 0.4861\n",
      "  Batch Loss: 0.6002\n",
      "  Batch Loss: 0.6364\n",
      "  Batch Loss: 0.5740\n",
      "  Batch Loss: 0.3510\n",
      "  Batch Loss: 0.4226\n",
      "  Batch Loss: 0.4505\n",
      "  Batch Loss: 0.4313\n",
      "  Batch Loss: 0.5303\n",
      "  Batch Loss: 0.5257\n",
      "  Batch Loss: 0.5252\n",
      "  Batch Loss: 0.5875\n",
      "  Batch Loss: 0.4922\n",
      "  Batch Loss: 0.5161\n",
      "  Batch Loss: 0.4583\n",
      "  Batch Loss: 0.5459\n",
      "  Batch Loss: 0.6230\n",
      "  Batch Loss: 0.5967\n",
      "  Batch Loss: 0.5173\n",
      "  Batch Loss: 0.4354\n",
      "  Batch Loss: 0.4940\n",
      "  Batch Loss: 0.4019\n",
      "  Batch Loss: 0.4302\n",
      "  Batch Loss: 0.6065\n",
      "  Batch Loss: 0.4285\n",
      "  Batch Loss: 0.6162\n",
      "  Batch Loss: 0.4614\n",
      "  Batch Loss: 0.6396\n",
      "  Batch Loss: 0.4933\n",
      "  Batch Loss: 0.5855\n",
      "  Batch Loss: 0.3943\n",
      "  Batch Loss: 0.4508\n",
      "  Batch Loss: 0.4361\n",
      "  Batch Loss: 0.4226\n",
      "  Batch Loss: 0.5113\n",
      "  Batch Loss: 0.4554\n",
      "  Batch Loss: 0.2986\n",
      "  Batch Loss: 0.5274\n",
      "  Batch Loss: 0.4550\n",
      "  Batch Loss: 0.5923\n",
      "  Batch Loss: 0.3667\n",
      "  Batch Loss: 0.5733\n",
      "  Batch Loss: 0.4530\n",
      "  Batch Loss: 0.5058\n",
      "  Batch Loss: 0.5418\n",
      "  Batch Loss: 0.5783\n",
      "  Batch Loss: 0.5159\n",
      "  Batch Loss: 0.3788\n",
      "  Batch Loss: 0.7365\n",
      "  Batch Loss: 0.4767\n",
      "  Batch Loss: 0.5837\n",
      "  Batch Loss: 0.5921\n",
      "  Batch Loss: 0.4583\n",
      "  Batch Loss: 0.6347\n",
      "  Batch Loss: 0.6189\n",
      "  Batch Loss: 0.4218\n",
      "  Batch Loss: 0.7018\n",
      "  Batch Loss: 0.5328\n",
      "  Batch Loss: 0.3422\n",
      "  Batch Loss: 0.4751\n",
      "  Batch Loss: 0.4444\n",
      "  Batch Loss: 0.5076\n",
      "  Batch Loss: 0.4662\n",
      "  Batch Loss: 0.5700\n",
      "  Batch Loss: 0.4471\n",
      "  Batch Loss: 0.4347\n",
      "  Batch Loss: 0.5239\n",
      "  Batch Loss: 0.3653\n",
      "  Batch Loss: 0.2489\n",
      "  Batch Loss: 0.5095\n",
      "  Batch Loss: 0.4930\n",
      "  Batch Loss: 0.4494\n",
      "  Batch Loss: 0.5604\n",
      "  Batch Loss: 0.4557\n",
      "  Batch Loss: 0.4492\n",
      "  Batch Loss: 0.4607\n",
      "  Batch Loss: 0.4774\n",
      "  Batch Loss: 0.4738\n",
      "  Batch Loss: 0.6227\n",
      "  Batch Loss: 0.6299\n",
      "  Batch Loss: 0.4428\n",
      "  Batch Loss: 0.3244\n",
      "  Batch Loss: 0.7821\n",
      "  Batch Loss: 0.4367\n",
      "  Batch Loss: 0.4421\n",
      "  Batch Loss: 0.5477\n",
      "  Batch Loss: 0.3831\n",
      "  Batch Loss: 0.4501\n",
      "  Batch Loss: 0.2727\n",
      "  Batch Loss: 0.4826\n",
      "  Batch Loss: 0.4447\n",
      "  Batch Loss: 0.4345\n",
      "  Batch Loss: 0.3530\n",
      "  Batch Loss: 0.6997\n",
      "  Batch Loss: 0.4410\n",
      "  Batch Loss: 0.4375\n",
      "  Batch Loss: 0.6305\n",
      "  Batch Loss: 0.4277\n",
      "  Batch Loss: 0.8618\n",
      "  Batch Loss: 0.3470\n",
      "  Batch Loss: 0.6117\n",
      "  Batch Loss: 0.4550\n",
      "  Batch Loss: 0.4027\n",
      "  Batch Loss: 0.3518\n",
      "  Batch Loss: 0.3648\n",
      "  Batch Loss: 0.3331\n",
      "  Batch Loss: 0.5160\n",
      "  Batch Loss: 0.7141\n",
      "  Batch Loss: 0.4548\n",
      "  Batch Loss: 0.5279\n",
      "  Batch Loss: 0.5514\n",
      "  Batch Loss: 0.5739\n",
      "  Batch Loss: 0.5775\n",
      "  Batch Loss: 0.7890\n",
      "  Batch Loss: 0.3411\n",
      "  Batch Loss: 0.5070\n",
      "  Batch Loss: 0.5009\n",
      "  Batch Loss: 0.3341\n",
      "  Batch Loss: 0.3247\n",
      "  Batch Loss: 0.5576\n",
      "  Batch Loss: 0.4248\n",
      "  Batch Loss: 0.5403\n",
      "  Batch Loss: 0.4226\n",
      "  Batch Loss: 0.6057\n",
      "  Batch Loss: 0.3119\n",
      "Average Loss: 0.6964\n",
      "Epoch: 3\n",
      "  Batch Loss: 0.4177\n",
      "  Batch Loss: 0.4521\n",
      "  Batch Loss: 0.4307\n",
      "  Batch Loss: 0.4202\n",
      "  Batch Loss: 0.3812\n",
      "  Batch Loss: 0.3869\n",
      "  Batch Loss: 0.4708\n",
      "  Batch Loss: 0.4273\n",
      "  Batch Loss: 0.4505\n",
      "  Batch Loss: 0.3166\n",
      "  Batch Loss: 0.2680\n",
      "  Batch Loss: 0.4475\n",
      "  Batch Loss: 0.4011\n",
      "  Batch Loss: 0.4260\n",
      "  Batch Loss: 0.4285\n",
      "  Batch Loss: 0.5706\n",
      "  Batch Loss: 0.4272\n",
      "  Batch Loss: 0.3661\n",
      "  Batch Loss: 0.5467\n",
      "  Batch Loss: 0.2835\n",
      "  Batch Loss: 0.4059\n",
      "  Batch Loss: 0.3889\n",
      "  Batch Loss: 0.4330\n",
      "  Batch Loss: 0.3870\n",
      "  Batch Loss: 0.3426\n",
      "  Batch Loss: 0.3592\n",
      "  Batch Loss: 0.4554\n",
      "  Batch Loss: 0.4416\n",
      "  Batch Loss: 0.4251\n",
      "  Batch Loss: 0.4324\n",
      "  Batch Loss: 0.4016\n",
      "  Batch Loss: 0.4739\n",
      "  Batch Loss: 0.5606\n",
      "  Batch Loss: 0.4411\n",
      "  Batch Loss: 0.4645\n",
      "  Batch Loss: 0.2853\n",
      "  Batch Loss: 0.3499\n",
      "  Batch Loss: 0.5251\n",
      "  Batch Loss: 0.2852\n",
      "  Batch Loss: 0.5091\n",
      "  Batch Loss: 0.2921\n",
      "  Batch Loss: 0.5284\n",
      "  Batch Loss: 0.5591\n",
      "  Batch Loss: 0.3563\n",
      "  Batch Loss: 0.4741\n",
      "  Batch Loss: 0.4259\n",
      "  Batch Loss: 0.2744\n",
      "  Batch Loss: 0.3598\n",
      "  Batch Loss: 0.4077\n",
      "  Batch Loss: 0.4980\n",
      "  Batch Loss: 0.4379\n",
      "  Batch Loss: 0.5213\n",
      "  Batch Loss: 0.3451\n",
      "  Batch Loss: 0.4413\n",
      "  Batch Loss: 0.4721\n",
      "  Batch Loss: 0.3813\n",
      "  Batch Loss: 0.3379\n",
      "  Batch Loss: 0.4063\n",
      "  Batch Loss: 0.4860\n",
      "  Batch Loss: 0.3423\n",
      "  Batch Loss: 0.3398\n",
      "  Batch Loss: 0.2762\n",
      "  Batch Loss: 0.5015\n",
      "  Batch Loss: 0.3239\n",
      "  Batch Loss: 0.4129\n",
      "  Batch Loss: 0.6521\n",
      "  Batch Loss: 0.3284\n",
      "  Batch Loss: 0.3212\n",
      "  Batch Loss: 0.3290\n",
      "  Batch Loss: 0.5392\n",
      "  Batch Loss: 0.4330\n",
      "  Batch Loss: 0.4662\n",
      "  Batch Loss: 0.3781\n",
      "  Batch Loss: 0.3269\n",
      "  Batch Loss: 0.3013\n",
      "  Batch Loss: 0.4524\n",
      "  Batch Loss: 0.4172\n",
      "  Batch Loss: 0.9246\n",
      "  Batch Loss: 0.4868\n",
      "  Batch Loss: 0.3721\n",
      "  Batch Loss: 0.3852\n",
      "  Batch Loss: 0.3062\n",
      "  Batch Loss: 0.5683\n",
      "  Batch Loss: 0.3477\n",
      "  Batch Loss: 0.4324\n",
      "  Batch Loss: 0.5250\n",
      "  Batch Loss: 0.3885\n",
      "  Batch Loss: 0.3331\n",
      "  Batch Loss: 0.5561\n",
      "  Batch Loss: 0.3945\n",
      "  Batch Loss: 0.4145\n",
      "  Batch Loss: 0.3854\n",
      "  Batch Loss: 0.4102\n",
      "  Batch Loss: 0.2973\n",
      "  Batch Loss: 0.3114\n",
      "  Batch Loss: 0.4861\n",
      "  Batch Loss: 0.3303\n",
      "  Batch Loss: 0.3472\n",
      "  Batch Loss: 0.4170\n",
      "  Batch Loss: 0.3568\n",
      "  Batch Loss: 0.3092\n",
      "  Batch Loss: 0.4785\n",
      "  Batch Loss: 0.5581\n",
      "  Batch Loss: 0.3499\n",
      "  Batch Loss: 0.3925\n",
      "  Batch Loss: 0.5333\n",
      "  Batch Loss: 0.2935\n",
      "  Batch Loss: 0.3877\n",
      "  Batch Loss: 0.4028\n",
      "  Batch Loss: 0.4593\n",
      "  Batch Loss: 0.3586\n",
      "  Batch Loss: 0.5074\n",
      "  Batch Loss: 0.4038\n",
      "  Batch Loss: 0.3928\n",
      "  Batch Loss: 0.3786\n",
      "  Batch Loss: 0.5377\n",
      "  Batch Loss: 0.4669\n",
      "  Batch Loss: 0.5526\n",
      "  Batch Loss: 0.4380\n",
      "  Batch Loss: 0.4364\n",
      "  Batch Loss: 0.4142\n",
      "  Batch Loss: 0.4201\n",
      "  Batch Loss: 0.3715\n",
      "  Batch Loss: 0.4601\n",
      "  Batch Loss: 0.4326\n",
      "  Batch Loss: 0.3504\n",
      "  Batch Loss: 0.5191\n",
      "  Batch Loss: 0.3001\n",
      "  Batch Loss: 0.3366\n",
      "  Batch Loss: 0.3924\n",
      "  Batch Loss: 0.3085\n",
      "  Batch Loss: 0.3768\n",
      "  Batch Loss: 0.3667\n",
      "  Batch Loss: 0.3488\n",
      "  Batch Loss: 0.3529\n",
      "  Batch Loss: 0.4627\n",
      "  Batch Loss: 0.3432\n",
      "  Batch Loss: 0.4457\n",
      "  Batch Loss: 0.5349\n",
      "  Batch Loss: 0.4475\n",
      "  Batch Loss: 0.4895\n",
      "  Batch Loss: 0.4645\n",
      "  Batch Loss: 0.3642\n",
      "  Batch Loss: 0.3541\n",
      "  Batch Loss: 0.4975\n",
      "  Batch Loss: 0.3170\n",
      "  Batch Loss: 0.4156\n",
      "  Batch Loss: 0.3332\n",
      "  Batch Loss: 0.5545\n",
      "  Batch Loss: 0.5871\n",
      "  Batch Loss: 0.3602\n",
      "  Batch Loss: 0.4678\n",
      "  Batch Loss: 0.5972\n",
      "  Batch Loss: 0.3170\n",
      "  Batch Loss: 0.5488\n",
      "  Batch Loss: 0.3900\n",
      "  Batch Loss: 0.3644\n",
      "  Batch Loss: 0.2924\n",
      "  Batch Loss: 0.3051\n",
      "  Batch Loss: 0.3836\n",
      "  Batch Loss: 0.4001\n",
      "  Batch Loss: 0.3315\n",
      "  Batch Loss: 0.4700\n",
      "  Batch Loss: 0.3213\n",
      "  Batch Loss: 0.3696\n",
      "  Batch Loss: 0.3367\n",
      "  Batch Loss: 0.5589\n",
      "  Batch Loss: 0.2576\n",
      "  Batch Loss: 0.3027\n",
      "  Batch Loss: 0.3472\n",
      "  Batch Loss: 0.3961\n",
      "  Batch Loss: 0.3720\n",
      "  Batch Loss: 0.3384\n",
      "  Batch Loss: 0.3698\n",
      "  Batch Loss: 0.3539\n",
      "  Batch Loss: 0.3333\n",
      "  Batch Loss: 0.2754\n",
      "  Batch Loss: 0.2283\n",
      "  Batch Loss: 0.1931\n",
      "  Batch Loss: 0.4214\n",
      "  Batch Loss: 0.3483\n",
      "  Batch Loss: 0.3227\n",
      "  Batch Loss: 0.3014\n",
      "  Batch Loss: 0.2887\n",
      "  Batch Loss: 0.3142\n",
      "  Batch Loss: 0.3217\n",
      "  Batch Loss: 0.3797\n",
      "  Batch Loss: 0.4604\n",
      "  Batch Loss: 0.4231\n",
      "  Batch Loss: 0.3212\n",
      "  Batch Loss: 0.3823\n",
      "  Batch Loss: 0.3358\n",
      "  Batch Loss: 0.5333\n",
      "  Batch Loss: 0.3803\n",
      "  Batch Loss: 0.4504\n",
      "  Batch Loss: 0.1910\n",
      "  Batch Loss: 0.5771\n",
      "  Batch Loss: 0.4434\n",
      "  Batch Loss: 0.5027\n",
      "  Batch Loss: 0.3979\n",
      "  Batch Loss: 0.3051\n",
      "  Batch Loss: 0.5503\n",
      "  Batch Loss: 0.3402\n",
      "  Batch Loss: 0.4315\n",
      "  Batch Loss: 0.4887\n",
      "  Batch Loss: 0.3618\n",
      "  Batch Loss: 0.3955\n",
      "  Batch Loss: 0.4956\n",
      "  Batch Loss: 0.4575\n",
      "  Batch Loss: 0.4612\n",
      "  Batch Loss: 0.3792\n",
      "  Batch Loss: 0.3775\n",
      "  Batch Loss: 0.3568\n",
      "  Batch Loss: 0.5146\n",
      "  Batch Loss: 0.3691\n",
      "  Batch Loss: 0.2729\n",
      "  Batch Loss: 0.3574\n",
      "  Batch Loss: 0.2803\n",
      "  Batch Loss: 0.3294\n",
      "  Batch Loss: 0.2921\n",
      "  Batch Loss: 0.4039\n",
      "  Batch Loss: 0.2668\n",
      "  Batch Loss: 0.3356\n",
      "  Batch Loss: 0.3222\n",
      "  Batch Loss: 0.5182\n",
      "  Batch Loss: 0.3260\n",
      "  Batch Loss: 0.4000\n",
      "  Batch Loss: 0.4394\n",
      "  Batch Loss: 0.5598\n",
      "  Batch Loss: 0.3718\n",
      "  Batch Loss: 0.3069\n",
      "  Batch Loss: 0.3124\n",
      "  Batch Loss: 0.3784\n",
      "  Batch Loss: 0.4106\n",
      "  Batch Loss: 0.2906\n",
      "  Batch Loss: 0.4259\n",
      "  Batch Loss: 0.3664\n",
      "  Batch Loss: 0.3470\n",
      "  Batch Loss: 0.4447\n",
      "  Batch Loss: 0.4276\n",
      "  Batch Loss: 0.3702\n",
      "  Batch Loss: 0.5216\n",
      "  Batch Loss: 0.4338\n",
      "  Batch Loss: 0.6224\n",
      "  Batch Loss: 0.4060\n",
      "  Batch Loss: 0.3811\n",
      "  Batch Loss: 0.2346\n",
      "  Batch Loss: 0.3879\n",
      "  Batch Loss: 0.4177\n",
      "  Batch Loss: 0.3324\n",
      "  Batch Loss: 0.3953\n",
      "  Batch Loss: 0.4683\n",
      "  Batch Loss: 0.4169\n",
      "  Batch Loss: 0.4014\n",
      "  Batch Loss: 0.4175\n",
      "  Batch Loss: 0.3831\n",
      "  Batch Loss: 0.3525\n",
      "  Batch Loss: 0.4001\n",
      "  Batch Loss: 0.3682\n",
      "  Batch Loss: 0.3666\n",
      "  Batch Loss: 0.3519\n",
      "  Batch Loss: 0.2934\n",
      "  Batch Loss: 0.4439\n",
      "  Batch Loss: 0.3094\n",
      "  Batch Loss: 0.3380\n",
      "  Batch Loss: 0.3764\n",
      "  Batch Loss: 0.3579\n",
      "  Batch Loss: 0.3469\n",
      "  Batch Loss: 0.2845\n",
      "  Batch Loss: 0.2872\n",
      "  Batch Loss: 0.3735\n",
      "  Batch Loss: 0.4789\n",
      "  Batch Loss: 0.4674\n",
      "  Batch Loss: 0.3001\n",
      "  Batch Loss: 0.2366\n",
      "  Batch Loss: 0.2630\n",
      "  Batch Loss: 0.3688\n",
      "  Batch Loss: 0.2706\n",
      "  Batch Loss: 0.4461\n",
      "  Batch Loss: 0.4207\n",
      "  Batch Loss: 0.3238\n",
      "  Batch Loss: 0.3605\n",
      "  Batch Loss: 0.3557\n",
      "  Batch Loss: 0.6364\n",
      "  Batch Loss: 0.2687\n",
      "  Batch Loss: 0.3522\n",
      "  Batch Loss: 0.3153\n",
      "  Batch Loss: 0.4607\n",
      "  Batch Loss: 0.3972\n",
      "  Batch Loss: 0.3227\n",
      "  Batch Loss: 0.5277\n",
      "  Batch Loss: 0.5399\n",
      "  Batch Loss: 0.3752\n",
      "  Batch Loss: 0.3942\n",
      "  Batch Loss: 0.3622\n",
      "  Batch Loss: 0.3495\n",
      "  Batch Loss: 0.4502\n",
      "  Batch Loss: 0.3949\n",
      "  Batch Loss: 0.4100\n",
      "  Batch Loss: 0.4287\n",
      "  Batch Loss: 0.3844\n",
      "  Batch Loss: 0.2398\n",
      "  Batch Loss: 0.3833\n",
      "  Batch Loss: 0.2747\n",
      "  Batch Loss: 0.4684\n",
      "  Batch Loss: 0.2975\n",
      "  Batch Loss: 0.4953\n",
      "  Batch Loss: 0.3650\n",
      "  Batch Loss: 0.4218\n",
      "  Batch Loss: 0.5266\n",
      "  Batch Loss: 0.3542\n",
      "  Batch Loss: 0.4779\n",
      "  Batch Loss: 0.2239\n",
      "  Batch Loss: 0.3176\n",
      "  Batch Loss: 0.2653\n",
      "  Batch Loss: 0.3284\n",
      "  Batch Loss: 0.4825\n",
      "  Batch Loss: 0.2766\n",
      "  Batch Loss: 0.3575\n",
      "  Batch Loss: 0.4583\n",
      "  Batch Loss: 0.3901\n",
      "  Batch Loss: 0.4367\n",
      "  Batch Loss: 0.4331\n",
      "  Batch Loss: 0.4511\n",
      "  Batch Loss: 0.1944\n",
      "  Batch Loss: 0.3788\n",
      "  Batch Loss: 0.4616\n",
      "  Batch Loss: 0.3872\n",
      "  Batch Loss: 0.3206\n",
      "  Batch Loss: 0.5049\n",
      "  Batch Loss: 0.3825\n",
      "  Batch Loss: 0.2637\n",
      "  Batch Loss: 0.3319\n",
      "  Batch Loss: 0.4254\n",
      "  Batch Loss: 0.3179\n",
      "  Batch Loss: 0.3543\n",
      "  Batch Loss: 0.2744\n",
      "  Batch Loss: 0.3312\n",
      "  Batch Loss: 0.2677\n",
      "  Batch Loss: 0.4468\n",
      "  Batch Loss: 0.2588\n",
      "  Batch Loss: 0.4043\n",
      "  Batch Loss: 0.3958\n",
      "  Batch Loss: 0.2903\n",
      "  Batch Loss: 0.3028\n",
      "  Batch Loss: 0.2943\n",
      "  Batch Loss: 0.4747\n",
      "  Batch Loss: 0.4749\n",
      "  Batch Loss: 0.4283\n",
      "  Batch Loss: 0.2816\n",
      "  Batch Loss: 0.6615\n",
      "  Batch Loss: 0.3178\n",
      "  Batch Loss: 0.2496\n",
      "  Batch Loss: 0.3400\n",
      "  Batch Loss: 0.2747\n",
      "  Batch Loss: 0.4030\n",
      "  Batch Loss: 0.3229\n",
      "  Batch Loss: 0.3602\n",
      "  Batch Loss: 0.2081\n",
      "  Batch Loss: 0.3442\n",
      "  Batch Loss: 0.2379\n",
      "  Batch Loss: 0.2080\n",
      "  Batch Loss: 0.3326\n",
      "  Batch Loss: 0.3668\n",
      "  Batch Loss: 0.2558\n",
      "  Batch Loss: 0.3520\n",
      "  Batch Loss: 0.3801\n",
      "  Batch Loss: 0.3861\n",
      "  Batch Loss: 0.2981\n",
      "  Batch Loss: 0.2575\n",
      "  Batch Loss: 0.4913\n",
      "  Batch Loss: 0.4733\n",
      "  Batch Loss: 0.4808\n",
      "  Batch Loss: 0.5081\n",
      "  Batch Loss: 0.3945\n",
      "  Batch Loss: 0.4062\n",
      "  Batch Loss: 0.3307\n",
      "  Batch Loss: 0.3086\n",
      "  Batch Loss: 0.3394\n",
      "  Batch Loss: 0.4281\n",
      "  Batch Loss: 0.3411\n",
      "  Batch Loss: 0.2354\n",
      "  Batch Loss: 0.2871\n",
      "  Batch Loss: 0.5864\n",
      "  Batch Loss: 0.3890\n",
      "  Batch Loss: 0.3572\n",
      "  Batch Loss: 0.4697\n",
      "  Batch Loss: 0.4329\n",
      "  Batch Loss: 0.3892\n",
      "  Batch Loss: 0.4518\n",
      "  Batch Loss: 0.4974\n",
      "  Batch Loss: 0.4023\n",
      "  Batch Loss: 0.4906\n",
      "  Batch Loss: 0.3098\n",
      "  Batch Loss: 0.3713\n",
      "  Batch Loss: 0.3408\n",
      "  Batch Loss: 0.4473\n",
      "  Batch Loss: 0.2740\n",
      "  Batch Loss: 0.2149\n",
      "  Batch Loss: 0.2325\n",
      "  Batch Loss: 0.2772\n",
      "  Batch Loss: 0.4402\n",
      "  Batch Loss: 0.3675\n",
      "  Batch Loss: 0.3010\n",
      "  Batch Loss: 0.3151\n",
      "  Batch Loss: 0.2566\n",
      "  Batch Loss: 0.3778\n",
      "  Batch Loss: 0.3148\n",
      "  Batch Loss: 0.5386\n",
      "  Batch Loss: 0.3719\n",
      "  Batch Loss: 0.4881\n",
      "  Batch Loss: 0.3003\n",
      "  Batch Loss: 0.3244\n",
      "  Batch Loss: 0.2769\n",
      "  Batch Loss: 0.3554\n",
      "  Batch Loss: 0.2878\n",
      "  Batch Loss: 0.2445\n",
      "  Batch Loss: 0.3171\n",
      "  Batch Loss: 0.2156\n",
      "  Batch Loss: 0.3338\n",
      "  Batch Loss: 0.3182\n",
      "  Batch Loss: 0.4156\n",
      "  Batch Loss: 0.3207\n",
      "  Batch Loss: 0.2638\n",
      "  Batch Loss: 0.3362\n",
      "  Batch Loss: 0.4608\n",
      "  Batch Loss: 0.4863\n",
      "  Batch Loss: 0.3007\n",
      "  Batch Loss: 0.3223\n",
      "  Batch Loss: 0.3759\n",
      "  Batch Loss: 0.3016\n",
      "  Batch Loss: 0.3235\n",
      "  Batch Loss: 0.2673\n",
      "  Batch Loss: 0.3091\n",
      "  Batch Loss: 0.2869\n",
      "  Batch Loss: 0.3098\n",
      "  Batch Loss: 0.3924\n",
      "  Batch Loss: 0.1844\n",
      "Average Loss: 0.3862\n",
      "Epoch: 4\n",
      "  Batch Loss: 0.4299\n",
      "  Batch Loss: 0.5201\n",
      "  Batch Loss: 0.2371\n",
      "  Batch Loss: 0.3744\n",
      "  Batch Loss: 0.4698\n",
      "  Batch Loss: 0.3122\n",
      "  Batch Loss: 0.2711\n",
      "  Batch Loss: 0.2765\n",
      "  Batch Loss: 0.1976\n",
      "  Batch Loss: 0.4146\n",
      "  Batch Loss: 0.3524\n",
      "  Batch Loss: 0.2665\n",
      "  Batch Loss: 0.2063\n",
      "  Batch Loss: 0.2807\n",
      "  Batch Loss: 0.3182\n",
      "  Batch Loss: 0.2656\n",
      "  Batch Loss: 0.2646\n",
      "  Batch Loss: 0.2237\n",
      "  Batch Loss: 0.2552\n",
      "  Batch Loss: 0.2550\n",
      "  Batch Loss: 0.3783\n",
      "  Batch Loss: 0.3275\n",
      "  Batch Loss: 0.3796\n",
      "  Batch Loss: 0.2413\n",
      "  Batch Loss: 0.1777\n",
      "  Batch Loss: 0.3006\n",
      "  Batch Loss: 0.4026\n",
      "  Batch Loss: 0.3971\n",
      "  Batch Loss: 0.5281\n",
      "  Batch Loss: 0.2769\n",
      "  Batch Loss: 0.2938\n",
      "  Batch Loss: 0.3723\n",
      "  Batch Loss: 0.2997\n",
      "  Batch Loss: 0.3760\n",
      "  Batch Loss: 0.2348\n",
      "  Batch Loss: 0.3468\n",
      "  Batch Loss: 0.2296\n",
      "  Batch Loss: 0.3427\n",
      "  Batch Loss: 0.4534\n",
      "  Batch Loss: 0.3658\n",
      "  Batch Loss: 0.2660\n",
      "  Batch Loss: 0.3063\n",
      "  Batch Loss: 0.3569\n",
      "  Batch Loss: 0.3380\n",
      "  Batch Loss: 0.3570\n",
      "  Batch Loss: 0.2381\n",
      "  Batch Loss: 0.3441\n",
      "  Batch Loss: 0.1808\n",
      "  Batch Loss: 0.2458\n",
      "  Batch Loss: 0.3492\n",
      "  Batch Loss: 0.2980\n",
      "  Batch Loss: 0.2822\n",
      "  Batch Loss: 0.3822\n",
      "  Batch Loss: 0.2846\n",
      "  Batch Loss: 0.2348\n",
      "  Batch Loss: 0.3961\n",
      "  Batch Loss: 0.3192\n",
      "  Batch Loss: 0.2498\n",
      "  Batch Loss: 0.4195\n",
      "  Batch Loss: 0.2512\n",
      "  Batch Loss: 0.3944\n",
      "  Batch Loss: 0.2990\n",
      "  Batch Loss: 0.4305\n",
      "  Batch Loss: 0.3506\n",
      "  Batch Loss: 0.2018\n",
      "  Batch Loss: 0.2285\n",
      "  Batch Loss: 0.3182\n",
      "  Batch Loss: 0.3099\n",
      "  Batch Loss: 0.2495\n",
      "  Batch Loss: 0.3234\n",
      "  Batch Loss: 0.2744\n",
      "  Batch Loss: 0.2901\n",
      "  Batch Loss: 0.2574\n",
      "  Batch Loss: 0.3010\n",
      "  Batch Loss: 0.2493\n",
      "  Batch Loss: 0.2236\n",
      "  Batch Loss: 0.3349\n",
      "  Batch Loss: 0.3095\n",
      "  Batch Loss: 0.3370\n",
      "  Batch Loss: 0.3697\n",
      "  Batch Loss: 0.3174\n",
      "  Batch Loss: 0.3196\n",
      "  Batch Loss: 0.3925\n",
      "  Batch Loss: 0.2215\n",
      "  Batch Loss: 0.3315\n",
      "  Batch Loss: 0.3061\n",
      "  Batch Loss: 0.3481\n",
      "  Batch Loss: 0.3412\n",
      "  Batch Loss: 0.3873\n",
      "  Batch Loss: 0.2838\n",
      "  Batch Loss: 0.3416\n",
      "  Batch Loss: 0.2730\n",
      "  Batch Loss: 0.3618\n",
      "  Batch Loss: 0.3068\n",
      "  Batch Loss: 0.3199\n",
      "  Batch Loss: 0.1885\n",
      "  Batch Loss: 0.2574\n",
      "  Batch Loss: 0.2691\n",
      "  Batch Loss: 0.3289\n",
      "  Batch Loss: 0.2705\n",
      "  Batch Loss: 0.2849\n",
      "  Batch Loss: 0.1855\n",
      "  Batch Loss: 0.3069\n",
      "  Batch Loss: 0.3169\n",
      "  Batch Loss: 0.2670\n",
      "  Batch Loss: 0.3660\n",
      "  Batch Loss: 0.2340\n",
      "  Batch Loss: 0.2380\n",
      "  Batch Loss: 0.3829\n",
      "  Batch Loss: 0.2074\n",
      "  Batch Loss: 0.2619\n",
      "  Batch Loss: 0.2294\n",
      "  Batch Loss: 0.3510\n",
      "  Batch Loss: 0.2813\n",
      "  Batch Loss: 0.2001\n",
      "  Batch Loss: 0.3669\n",
      "  Batch Loss: 0.2499\n",
      "  Batch Loss: 0.2938\n",
      "  Batch Loss: 0.2106\n",
      "  Batch Loss: 0.3652\n",
      "  Batch Loss: 0.3678\n",
      "  Batch Loss: 0.3550\n",
      "  Batch Loss: 0.2369\n",
      "  Batch Loss: 0.2639\n",
      "  Batch Loss: 0.3096\n",
      "  Batch Loss: 0.2842\n",
      "  Batch Loss: 0.3222\n",
      "  Batch Loss: 0.4584\n",
      "  Batch Loss: 0.2779\n",
      "  Batch Loss: 0.2768\n",
      "  Batch Loss: 0.2615\n",
      "  Batch Loss: 0.3372\n",
      "  Batch Loss: 0.2029\n",
      "  Batch Loss: 0.2747\n",
      "  Batch Loss: 0.2571\n",
      "  Batch Loss: 0.3505\n",
      "  Batch Loss: 0.2479\n",
      "  Batch Loss: 0.2834\n",
      "  Batch Loss: 0.2996\n",
      "  Batch Loss: 0.4347\n",
      "  Batch Loss: 0.2949\n",
      "  Batch Loss: 0.2668\n",
      "  Batch Loss: 0.2138\n",
      "  Batch Loss: 0.3580\n",
      "  Batch Loss: 0.2745\n",
      "  Batch Loss: 0.3016\n",
      "  Batch Loss: 0.4917\n",
      "  Batch Loss: 0.3209\n",
      "  Batch Loss: 0.3668\n",
      "  Batch Loss: 0.3494\n",
      "  Batch Loss: 0.1973\n",
      "  Batch Loss: 0.3562\n",
      "  Batch Loss: 0.3155\n",
      "  Batch Loss: 0.3845\n",
      "  Batch Loss: 0.4255\n",
      "  Batch Loss: 0.1779\n",
      "  Batch Loss: 0.3544\n",
      "  Batch Loss: 0.3576\n",
      "  Batch Loss: 0.2107\n",
      "  Batch Loss: 0.4377\n",
      "  Batch Loss: 0.3726\n",
      "  Batch Loss: 0.2113\n",
      "  Batch Loss: 0.3745\n",
      "  Batch Loss: 0.2958\n",
      "  Batch Loss: 0.3306\n",
      "  Batch Loss: 0.3071\n",
      "  Batch Loss: 0.1839\n",
      "  Batch Loss: 0.3192\n",
      "  Batch Loss: 0.4320\n",
      "  Batch Loss: 0.4236\n",
      "  Batch Loss: 0.2891\n",
      "  Batch Loss: 0.3107\n",
      "  Batch Loss: 0.3575\n",
      "  Batch Loss: 0.2618\n",
      "  Batch Loss: 0.3073\n",
      "  Batch Loss: 0.3636\n",
      "  Batch Loss: 0.3020\n",
      "  Batch Loss: 0.3695\n",
      "  Batch Loss: 0.2718\n",
      "  Batch Loss: 0.3511\n",
      "  Batch Loss: 0.3100\n",
      "  Batch Loss: 0.4700\n",
      "  Batch Loss: 0.2933\n",
      "  Batch Loss: 0.3212\n",
      "  Batch Loss: 0.3546\n",
      "  Batch Loss: 0.3131\n",
      "  Batch Loss: 0.2999\n",
      "  Batch Loss: 0.2453\n",
      "  Batch Loss: 0.2914\n",
      "  Batch Loss: 0.2706\n",
      "  Batch Loss: 0.3673\n",
      "  Batch Loss: 0.2490\n",
      "  Batch Loss: 0.2501\n",
      "  Batch Loss: 0.3108\n",
      "  Batch Loss: 0.2436\n",
      "  Batch Loss: 0.3542\n",
      "  Batch Loss: 0.2720\n",
      "  Batch Loss: 0.2615\n",
      "  Batch Loss: 0.6293\n",
      "  Batch Loss: 0.3302\n",
      "  Batch Loss: 0.3410\n",
      "  Batch Loss: 0.2044\n",
      "  Batch Loss: 0.2681\n",
      "  Batch Loss: 0.2319\n",
      "  Batch Loss: 0.2382\n",
      "  Batch Loss: 0.2748\n",
      "  Batch Loss: 0.3234\n",
      "  Batch Loss: 0.2018\n",
      "  Batch Loss: 0.2695\n",
      "  Batch Loss: 0.2232\n",
      "  Batch Loss: 0.2689\n",
      "  Batch Loss: 0.2273\n",
      "  Batch Loss: 0.4123\n",
      "  Batch Loss: 0.2799\n",
      "  Batch Loss: 0.2235\n",
      "  Batch Loss: 0.3249\n",
      "  Batch Loss: 0.2968\n",
      "  Batch Loss: 0.2741\n",
      "  Batch Loss: 0.2564\n",
      "  Batch Loss: 0.2250\n",
      "  Batch Loss: 0.3115\n",
      "  Batch Loss: 0.1773\n",
      "  Batch Loss: 0.2620\n",
      "  Batch Loss: 0.3164\n",
      "  Batch Loss: 0.3696\n",
      "  Batch Loss: 0.3772\n",
      "  Batch Loss: 0.2284\n",
      "  Batch Loss: 0.2576\n",
      "  Batch Loss: 0.3350\n",
      "  Batch Loss: 0.2278\n",
      "  Batch Loss: 0.2548\n",
      "  Batch Loss: 0.3107\n",
      "  Batch Loss: 0.2524\n",
      "  Batch Loss: 0.3357\n",
      "  Batch Loss: 0.3199\n",
      "  Batch Loss: 0.2501\n",
      "  Batch Loss: 0.2975\n",
      "  Batch Loss: 0.2418\n",
      "  Batch Loss: 0.3817\n",
      "  Batch Loss: 0.1833\n",
      "  Batch Loss: 0.3338\n",
      "  Batch Loss: 0.2650\n",
      "  Batch Loss: 0.4596\n",
      "  Batch Loss: 0.3791\n",
      "  Batch Loss: 0.3104\n",
      "  Batch Loss: 0.2150\n",
      "  Batch Loss: 0.3098\n",
      "  Batch Loss: 0.5057\n",
      "  Batch Loss: 0.2362\n",
      "  Batch Loss: 0.2667\n",
      "  Batch Loss: 0.2207\n",
      "  Batch Loss: 0.3214\n",
      "  Batch Loss: 0.3469\n",
      "  Batch Loss: 0.3404\n",
      "  Batch Loss: 0.3618\n",
      "  Batch Loss: 0.2382\n",
      "  Batch Loss: 0.2949\n",
      "  Batch Loss: 0.2109\n",
      "  Batch Loss: 0.4512\n",
      "  Batch Loss: 0.1670\n",
      "  Batch Loss: 0.2885\n",
      "  Batch Loss: 0.2609\n",
      "  Batch Loss: 0.3908\n",
      "  Batch Loss: 0.3326\n",
      "  Batch Loss: 0.2229\n",
      "  Batch Loss: 0.2812\n",
      "  Batch Loss: 0.2122\n",
      "  Batch Loss: 0.4043\n",
      "  Batch Loss: 0.2191\n",
      "  Batch Loss: 0.2685\n",
      "  Batch Loss: 0.3693\n",
      "  Batch Loss: 0.2579\n",
      "  Batch Loss: 0.2666\n",
      "  Batch Loss: 0.2786\n",
      "  Batch Loss: 0.3159\n",
      "  Batch Loss: 0.2840\n",
      "  Batch Loss: 0.3026\n",
      "  Batch Loss: 0.2326\n",
      "  Batch Loss: 0.2785\n",
      "  Batch Loss: 0.3302\n",
      "  Batch Loss: 0.4450\n",
      "  Batch Loss: 0.4069\n",
      "  Batch Loss: 0.3360\n",
      "  Batch Loss: 0.2707\n",
      "  Batch Loss: 0.2955\n",
      "  Batch Loss: 0.2507\n",
      "  Batch Loss: 0.2799\n",
      "  Batch Loss: 0.2511\n",
      "  Batch Loss: 0.2584\n",
      "  Batch Loss: 0.3222\n",
      "  Batch Loss: 0.2990\n",
      "  Batch Loss: 0.2051\n",
      "  Batch Loss: 0.3097\n",
      "  Batch Loss: 0.2652\n",
      "  Batch Loss: 0.2985\n",
      "  Batch Loss: 0.2587\n",
      "  Batch Loss: 0.3778\n",
      "  Batch Loss: 0.2819\n",
      "  Batch Loss: 0.3257\n",
      "  Batch Loss: 0.2819\n",
      "  Batch Loss: 0.2816\n",
      "  Batch Loss: 0.2671\n",
      "  Batch Loss: 0.1880\n",
      "  Batch Loss: 0.2789\n",
      "  Batch Loss: 0.3071\n",
      "  Batch Loss: 0.2792\n",
      "  Batch Loss: 0.2256\n",
      "  Batch Loss: 0.2490\n",
      "  Batch Loss: 0.3116\n",
      "  Batch Loss: 0.2459\n",
      "  Batch Loss: 0.2303\n",
      "  Batch Loss: 0.3392\n",
      "  Batch Loss: 0.2587\n",
      "  Batch Loss: 0.2112\n",
      "  Batch Loss: 0.2103\n",
      "  Batch Loss: 0.1905\n",
      "  Batch Loss: 0.3357\n",
      "  Batch Loss: 0.3302\n",
      "  Batch Loss: 0.2561\n",
      "  Batch Loss: 0.2640\n",
      "  Batch Loss: 0.2834\n",
      "  Batch Loss: 0.2044\n",
      "  Batch Loss: 0.2815\n",
      "  Batch Loss: 0.2380\n",
      "  Batch Loss: 0.2460\n",
      "  Batch Loss: 0.3260\n",
      "  Batch Loss: 0.3178\n",
      "  Batch Loss: 0.4257\n",
      "  Batch Loss: 0.2751\n",
      "  Batch Loss: 0.1659\n",
      "  Batch Loss: 0.2567\n",
      "  Batch Loss: 0.3763\n",
      "  Batch Loss: 0.2457\n",
      "  Batch Loss: 0.2222\n",
      "  Batch Loss: 0.1675\n",
      "  Batch Loss: 0.3789\n",
      "  Batch Loss: 0.3897\n",
      "  Batch Loss: 0.2444\n",
      "  Batch Loss: 0.3219\n",
      "  Batch Loss: 0.2650\n",
      "  Batch Loss: 0.2273\n",
      "  Batch Loss: 0.3367\n",
      "  Batch Loss: 0.3083\n",
      "  Batch Loss: 0.3203\n",
      "  Batch Loss: 0.3076\n",
      "  Batch Loss: 0.2782\n",
      "  Batch Loss: 0.3735\n",
      "  Batch Loss: 0.2465\n",
      "  Batch Loss: 0.2691\n",
      "  Batch Loss: 0.2209\n",
      "  Batch Loss: 0.2678\n",
      "  Batch Loss: 0.2499\n",
      "  Batch Loss: 0.2082\n",
      "  Batch Loss: 0.3629\n",
      "  Batch Loss: 0.4715\n",
      "  Batch Loss: 0.2174\n",
      "  Batch Loss: 0.2933\n",
      "  Batch Loss: 0.2954\n",
      "  Batch Loss: 0.2200\n",
      "  Batch Loss: 0.3797\n",
      "  Batch Loss: 0.2898\n",
      "  Batch Loss: 0.2956\n",
      "  Batch Loss: 0.1635\n",
      "  Batch Loss: 0.3048\n",
      "  Batch Loss: 0.2890\n",
      "  Batch Loss: 0.3351\n",
      "  Batch Loss: 0.2618\n",
      "  Batch Loss: 0.2611\n",
      "  Batch Loss: 0.3998\n",
      "  Batch Loss: 0.3766\n",
      "  Batch Loss: 0.2349\n",
      "  Batch Loss: 0.2040\n",
      "  Batch Loss: 0.2947\n",
      "  Batch Loss: 0.2757\n",
      "  Batch Loss: 0.2686\n",
      "  Batch Loss: 0.2434\n",
      "  Batch Loss: 0.2480\n",
      "  Batch Loss: 0.3751\n",
      "  Batch Loss: 0.3094\n",
      "  Batch Loss: 0.2686\n",
      "  Batch Loss: 0.2891\n",
      "  Batch Loss: 0.3419\n",
      "  Batch Loss: 0.3579\n",
      "  Batch Loss: 0.3724\n",
      "  Batch Loss: 0.3586\n",
      "  Batch Loss: 0.3059\n",
      "  Batch Loss: 0.2311\n",
      "  Batch Loss: 0.4281\n",
      "  Batch Loss: 0.1843\n",
      "  Batch Loss: 0.2663\n",
      "  Batch Loss: 0.3891\n",
      "  Batch Loss: 0.2469\n",
      "  Batch Loss: 0.2866\n",
      "  Batch Loss: 0.2685\n",
      "  Batch Loss: 0.3155\n",
      "  Batch Loss: 0.5041\n",
      "  Batch Loss: 0.2812\n",
      "  Batch Loss: 0.3400\n",
      "  Batch Loss: 0.4058\n",
      "  Batch Loss: 0.2899\n",
      "  Batch Loss: 0.2662\n",
      "  Batch Loss: 0.2220\n",
      "  Batch Loss: 0.4247\n",
      "  Batch Loss: 0.2705\n",
      "  Batch Loss: 0.3757\n",
      "  Batch Loss: 0.2074\n",
      "  Batch Loss: 0.1995\n",
      "  Batch Loss: 0.2599\n",
      "  Batch Loss: 0.2695\n",
      "  Batch Loss: 0.2921\n",
      "  Batch Loss: 0.3363\n",
      "  Batch Loss: 0.3121\n",
      "  Batch Loss: 0.3041\n",
      "  Batch Loss: 0.3183\n",
      "  Batch Loss: 0.2800\n",
      "  Batch Loss: 0.2072\n",
      "  Batch Loss: 0.2892\n",
      "  Batch Loss: 0.2830\n",
      "  Batch Loss: 0.2630\n",
      "  Batch Loss: 0.2402\n",
      "  Batch Loss: 0.2995\n",
      "  Batch Loss: 0.3251\n",
      "  Batch Loss: 0.3576\n",
      "  Batch Loss: 0.2664\n",
      "  Batch Loss: 0.2884\n",
      "  Batch Loss: 0.2543\n",
      "  Batch Loss: 0.2446\n",
      "  Batch Loss: 0.2465\n",
      "  Batch Loss: 0.3503\n",
      "  Batch Loss: 0.2073\n",
      "  Batch Loss: 0.3718\n",
      "  Batch Loss: 0.2255\n",
      "  Batch Loss: 0.2125\n",
      "  Batch Loss: 0.2038\n",
      "  Batch Loss: 0.2040\n",
      "  Batch Loss: 0.3549\n",
      "  Batch Loss: 0.2734\n",
      "  Batch Loss: 0.3841\n",
      "Average Loss: 0.2990\n",
      "Epoch: 5\n",
      "  Batch Loss: 0.2372\n",
      "  Batch Loss: 0.1462\n",
      "  Batch Loss: 0.2075\n",
      "  Batch Loss: 0.2188\n",
      "  Batch Loss: 0.1991\n",
      "  Batch Loss: 0.2050\n",
      "  Batch Loss: 0.3353\n",
      "  Batch Loss: 0.2947\n",
      "  Batch Loss: 0.2897\n",
      "  Batch Loss: 0.1980\n",
      "  Batch Loss: 0.1783\n",
      "  Batch Loss: 0.2404\n",
      "  Batch Loss: 0.1734\n",
      "  Batch Loss: 0.3057\n",
      "  Batch Loss: 0.3258\n",
      "  Batch Loss: 0.1848\n",
      "  Batch Loss: 0.3056\n",
      "  Batch Loss: 0.2781\n",
      "  Batch Loss: 0.1927\n",
      "  Batch Loss: 0.2373\n",
      "  Batch Loss: 0.3159\n",
      "  Batch Loss: 0.3710\n",
      "  Batch Loss: 0.2070\n",
      "  Batch Loss: 0.1697\n",
      "  Batch Loss: 0.3536\n",
      "  Batch Loss: 0.2879\n",
      "  Batch Loss: 0.3719\n",
      "  Batch Loss: 0.1605\n",
      "  Batch Loss: 0.2296\n",
      "  Batch Loss: 0.2935\n",
      "  Batch Loss: 0.2458\n",
      "  Batch Loss: 0.2456\n",
      "  Batch Loss: 0.3712\n",
      "  Batch Loss: 0.3080\n",
      "  Batch Loss: 0.2569\n",
      "  Batch Loss: 0.2270\n",
      "  Batch Loss: 0.3673\n",
      "  Batch Loss: 0.1897\n",
      "  Batch Loss: 0.3253\n",
      "  Batch Loss: 0.2483\n",
      "  Batch Loss: 0.4129\n",
      "  Batch Loss: 0.2361\n",
      "  Batch Loss: 0.3033\n",
      "  Batch Loss: 0.2055\n",
      "  Batch Loss: 0.2014\n",
      "  Batch Loss: 0.2476\n",
      "  Batch Loss: 0.3069\n",
      "  Batch Loss: 0.3203\n",
      "  Batch Loss: 0.2376\n",
      "  Batch Loss: 0.2596\n",
      "  Batch Loss: 0.2355\n",
      "  Batch Loss: 0.2594\n",
      "  Batch Loss: 0.2407\n",
      "  Batch Loss: 0.2173\n",
      "  Batch Loss: 0.3192\n",
      "  Batch Loss: 0.2855\n",
      "  Batch Loss: 0.3109\n",
      "  Batch Loss: 0.2799\n",
      "  Batch Loss: 0.2724\n",
      "  Batch Loss: 0.2069\n",
      "  Batch Loss: 0.2705\n",
      "  Batch Loss: 0.2782\n",
      "  Batch Loss: 0.3280\n",
      "  Batch Loss: 0.3348\n",
      "  Batch Loss: 0.3111\n",
      "  Batch Loss: 0.3185\n",
      "  Batch Loss: 0.1940\n",
      "  Batch Loss: 0.2095\n",
      "  Batch Loss: 0.1810\n",
      "  Batch Loss: 0.1615\n",
      "  Batch Loss: 0.1421\n",
      "  Batch Loss: 0.2378\n",
      "  Batch Loss: 0.1909\n",
      "  Batch Loss: 0.2135\n",
      "  Batch Loss: 0.2074\n",
      "  Batch Loss: 0.3151\n",
      "  Batch Loss: 0.1692\n",
      "  Batch Loss: 0.2515\n",
      "  Batch Loss: 0.2088\n",
      "  Batch Loss: 0.2818\n",
      "  Batch Loss: 0.3385\n",
      "  Batch Loss: 0.2538\n",
      "  Batch Loss: 0.3424\n",
      "  Batch Loss: 0.1865\n",
      "  Batch Loss: 0.2540\n",
      "  Batch Loss: 0.3195\n",
      "  Batch Loss: 0.2122\n",
      "  Batch Loss: 0.1816\n",
      "  Batch Loss: 0.3190\n",
      "  Batch Loss: 0.2489\n",
      "  Batch Loss: 0.2442\n",
      "  Batch Loss: 0.3551\n",
      "  Batch Loss: 0.2535\n",
      "  Batch Loss: 0.1962\n",
      "  Batch Loss: 0.2317\n",
      "  Batch Loss: 0.2598\n",
      "  Batch Loss: 0.3177\n",
      "  Batch Loss: 0.2647\n",
      "  Batch Loss: 0.2718\n",
      "  Batch Loss: 0.2931\n",
      "  Batch Loss: 0.2225\n",
      "  Batch Loss: 0.4510\n",
      "  Batch Loss: 0.3241\n",
      "  Batch Loss: 0.3060\n",
      "  Batch Loss: 0.2792\n",
      "  Batch Loss: 0.1278\n",
      "  Batch Loss: 0.1510\n",
      "  Batch Loss: 0.2247\n",
      "  Batch Loss: 0.2507\n",
      "  Batch Loss: 0.2974\n",
      "  Batch Loss: 0.2685\n",
      "  Batch Loss: 0.3032\n",
      "  Batch Loss: 0.2509\n",
      "  Batch Loss: 0.1944\n",
      "  Batch Loss: 0.2156\n",
      "  Batch Loss: 0.1931\n",
      "  Batch Loss: 0.1534\n",
      "  Batch Loss: 0.2272\n",
      "  Batch Loss: 0.2280\n",
      "  Batch Loss: 0.1978\n",
      "  Batch Loss: 0.2486\n",
      "  Batch Loss: 0.3102\n",
      "  Batch Loss: 0.2318\n",
      "  Batch Loss: 0.2776\n",
      "  Batch Loss: 0.2260\n",
      "  Batch Loss: 0.2678\n",
      "  Batch Loss: 0.3109\n",
      "  Batch Loss: 0.3143\n",
      "  Batch Loss: 0.2343\n",
      "  Batch Loss: 0.2537\n",
      "  Batch Loss: 0.2343\n",
      "  Batch Loss: 0.2936\n",
      "  Batch Loss: 0.3024\n",
      "  Batch Loss: 0.1508\n",
      "  Batch Loss: 0.2647\n",
      "  Batch Loss: 0.2422\n",
      "  Batch Loss: 0.3144\n",
      "  Batch Loss: 0.2947\n",
      "  Batch Loss: 0.3112\n",
      "  Batch Loss: 0.1797\n",
      "  Batch Loss: 0.2257\n",
      "  Batch Loss: 0.2288\n",
      "  Batch Loss: 0.2182\n",
      "  Batch Loss: 0.2169\n",
      "  Batch Loss: 0.2830\n",
      "  Batch Loss: 0.3542\n",
      "  Batch Loss: 0.1837\n",
      "  Batch Loss: 0.1860\n",
      "  Batch Loss: 0.2553\n",
      "  Batch Loss: 0.1501\n",
      "  Batch Loss: 0.2247\n",
      "  Batch Loss: 0.3062\n",
      "  Batch Loss: 0.2215\n",
      "  Batch Loss: 0.3092\n",
      "  Batch Loss: 0.2535\n",
      "  Batch Loss: 0.2508\n",
      "  Batch Loss: 0.1751\n",
      "  Batch Loss: 0.1668\n",
      "  Batch Loss: 0.2306\n",
      "  Batch Loss: 0.2222\n",
      "  Batch Loss: 0.1811\n",
      "  Batch Loss: 0.2277\n",
      "  Batch Loss: 0.3349\n",
      "  Batch Loss: 0.3508\n",
      "  Batch Loss: 0.2507\n",
      "  Batch Loss: 0.1518\n",
      "  Batch Loss: 0.2481\n",
      "  Batch Loss: 0.1949\n",
      "  Batch Loss: 0.2384\n",
      "  Batch Loss: 0.3026\n",
      "  Batch Loss: 0.3109\n",
      "  Batch Loss: 0.2977\n",
      "  Batch Loss: 0.1858\n",
      "  Batch Loss: 0.2130\n",
      "  Batch Loss: 0.1725\n",
      "  Batch Loss: 0.2381\n",
      "  Batch Loss: 0.1533\n",
      "  Batch Loss: 0.2549\n",
      "  Batch Loss: 0.2356\n",
      "  Batch Loss: 0.2719\n",
      "  Batch Loss: 0.3785\n",
      "  Batch Loss: 0.2904\n",
      "  Batch Loss: 0.2515\n",
      "  Batch Loss: 0.2157\n",
      "  Batch Loss: 0.1952\n",
      "  Batch Loss: 0.1644\n",
      "  Batch Loss: 0.3015\n",
      "  Batch Loss: 0.3340\n",
      "  Batch Loss: 0.2267\n",
      "  Batch Loss: 0.2082\n",
      "  Batch Loss: 0.2056\n",
      "  Batch Loss: 0.3413\n",
      "  Batch Loss: 0.3714\n",
      "  Batch Loss: 0.2657\n",
      "  Batch Loss: 0.2781\n",
      "  Batch Loss: 0.2580\n",
      "  Batch Loss: 0.1625\n",
      "  Batch Loss: 0.2364\n",
      "  Batch Loss: 0.2278\n",
      "  Batch Loss: 0.3434\n",
      "  Batch Loss: 0.2195\n",
      "  Batch Loss: 0.2440\n",
      "  Batch Loss: 0.1670\n",
      "  Batch Loss: 0.2104\n",
      "  Batch Loss: 0.1903\n",
      "  Batch Loss: 0.2951\n",
      "  Batch Loss: 0.2484\n",
      "  Batch Loss: 0.2945\n",
      "  Batch Loss: 0.1964\n",
      "  Batch Loss: 0.1929\n",
      "  Batch Loss: 0.2880\n",
      "  Batch Loss: 0.1852\n",
      "  Batch Loss: 0.1812\n",
      "  Batch Loss: 0.1875\n",
      "  Batch Loss: 0.2200\n",
      "  Batch Loss: 0.2014\n",
      "  Batch Loss: 0.2685\n",
      "  Batch Loss: 0.2681\n",
      "  Batch Loss: 0.2375\n",
      "  Batch Loss: 0.2793\n",
      "  Batch Loss: 0.2150\n",
      "  Batch Loss: 0.1916\n",
      "  Batch Loss: 0.3138\n",
      "  Batch Loss: 0.1090\n",
      "  Batch Loss: 0.3028\n",
      "  Batch Loss: 0.2561\n",
      "  Batch Loss: 0.3011\n",
      "  Batch Loss: 0.1398\n",
      "  Batch Loss: 0.2672\n",
      "  Batch Loss: 0.2307\n",
      "  Batch Loss: 0.2270\n",
      "  Batch Loss: 0.3095\n",
      "  Batch Loss: 0.4059\n",
      "  Batch Loss: 0.2366\n",
      "  Batch Loss: 0.2603\n",
      "  Batch Loss: 0.2174\n",
      "  Batch Loss: 0.3346\n",
      "  Batch Loss: 0.1754\n",
      "  Batch Loss: 0.2912\n",
      "  Batch Loss: 0.1845\n",
      "  Batch Loss: 0.2170\n",
      "  Batch Loss: 0.3383\n",
      "  Batch Loss: 0.1544\n",
      "  Batch Loss: 0.2751\n",
      "  Batch Loss: 0.2354\n",
      "  Batch Loss: 0.1904\n",
      "  Batch Loss: 0.1814\n",
      "  Batch Loss: 0.3086\n",
      "  Batch Loss: 0.2329\n",
      "  Batch Loss: 0.2347\n",
      "  Batch Loss: 0.2380\n",
      "  Batch Loss: 0.3130\n",
      "  Batch Loss: 0.1850\n",
      "  Batch Loss: 0.2045\n",
      "  Batch Loss: 0.1457\n",
      "  Batch Loss: 0.3275\n",
      "  Batch Loss: 0.2442\n",
      "  Batch Loss: 0.1698\n",
      "  Batch Loss: 0.1874\n",
      "  Batch Loss: 0.3111\n",
      "  Batch Loss: 0.1818\n",
      "  Batch Loss: 0.2308\n",
      "  Batch Loss: 0.3147\n",
      "  Batch Loss: 0.2423\n",
      "  Batch Loss: 0.2423\n",
      "  Batch Loss: 0.1905\n",
      "  Batch Loss: 0.1935\n",
      "  Batch Loss: 0.2448\n",
      "  Batch Loss: 0.1882\n",
      "  Batch Loss: 0.2634\n",
      "  Batch Loss: 0.2683\n",
      "  Batch Loss: 0.2816\n",
      "  Batch Loss: 0.1452\n",
      "  Batch Loss: 0.1936\n",
      "  Batch Loss: 0.1775\n",
      "  Batch Loss: 0.2368\n",
      "  Batch Loss: 0.2544\n",
      "  Batch Loss: 0.2792\n",
      "  Batch Loss: 0.2912\n",
      "  Batch Loss: 0.3115\n",
      "  Batch Loss: 0.3049\n",
      "  Batch Loss: 0.2359\n",
      "  Batch Loss: 0.2199\n",
      "  Batch Loss: 0.1733\n",
      "  Batch Loss: 0.2375\n",
      "  Batch Loss: 0.1855\n",
      "  Batch Loss: 0.3063\n",
      "  Batch Loss: 0.2989\n",
      "  Batch Loss: 0.3349\n",
      "  Batch Loss: 0.2402\n",
      "  Batch Loss: 0.2673\n",
      "  Batch Loss: 0.2203\n",
      "  Batch Loss: 0.2055\n",
      "  Batch Loss: 0.1718\n",
      "  Batch Loss: 0.2697\n",
      "  Batch Loss: 0.1501\n",
      "  Batch Loss: 0.2276\n",
      "  Batch Loss: 0.1259\n",
      "  Batch Loss: 0.2466\n",
      "  Batch Loss: 0.1673\n",
      "  Batch Loss: 0.2267\n",
      "  Batch Loss: 0.3662\n",
      "  Batch Loss: 0.2929\n",
      "  Batch Loss: 0.2234\n",
      "  Batch Loss: 0.1916\n",
      "  Batch Loss: 0.2317\n",
      "  Batch Loss: 0.2114\n",
      "  Batch Loss: 0.2579\n",
      "  Batch Loss: 0.2777\n",
      "  Batch Loss: 0.2236\n",
      "  Batch Loss: 0.2373\n",
      "  Batch Loss: 0.2577\n",
      "  Batch Loss: 0.2500\n",
      "  Batch Loss: 0.3694\n",
      "  Batch Loss: 0.2044\n",
      "  Batch Loss: 0.1740\n",
      "  Batch Loss: 0.1780\n",
      "  Batch Loss: 0.2507\n",
      "  Batch Loss: 0.2894\n",
      "  Batch Loss: 0.2877\n",
      "  Batch Loss: 0.3534\n",
      "  Batch Loss: 0.2646\n",
      "  Batch Loss: 0.2939\n",
      "  Batch Loss: 0.1918\n",
      "  Batch Loss: 0.2565\n",
      "  Batch Loss: 0.3100\n",
      "  Batch Loss: 0.2632\n",
      "  Batch Loss: 0.1823\n",
      "  Batch Loss: 0.2406\n",
      "  Batch Loss: 0.2298\n",
      "  Batch Loss: 0.2102\n",
      "  Batch Loss: 0.2024\n",
      "  Batch Loss: 0.1956\n",
      "  Batch Loss: 0.3078\n",
      "  Batch Loss: 0.3233\n",
      "  Batch Loss: 0.1403\n",
      "  Batch Loss: 0.2506\n",
      "  Batch Loss: 0.3033\n",
      "  Batch Loss: 0.4178\n",
      "  Batch Loss: 0.2748\n",
      "  Batch Loss: 0.2525\n",
      "  Batch Loss: 0.3737\n",
      "  Batch Loss: 0.2200\n",
      "  Batch Loss: 0.3413\n",
      "  Batch Loss: 0.3062\n",
      "  Batch Loss: 0.1588\n",
      "  Batch Loss: 0.2266\n",
      "  Batch Loss: 0.2257\n",
      "  Batch Loss: 0.2755\n",
      "  Batch Loss: 0.2325\n",
      "  Batch Loss: 0.2602\n",
      "  Batch Loss: 0.2966\n",
      "  Batch Loss: 0.2386\n",
      "  Batch Loss: 0.2757\n",
      "  Batch Loss: 0.2521\n",
      "  Batch Loss: 0.2042\n",
      "  Batch Loss: 0.2052\n",
      "  Batch Loss: 0.1957\n",
      "  Batch Loss: 0.1992\n",
      "  Batch Loss: 0.2567\n",
      "  Batch Loss: 0.2132\n",
      "  Batch Loss: 0.1868\n",
      "  Batch Loss: 0.2211\n",
      "  Batch Loss: 0.2176\n",
      "  Batch Loss: 0.1866\n",
      "  Batch Loss: 0.3209\n",
      "  Batch Loss: 0.1848\n",
      "  Batch Loss: 0.2210\n",
      "  Batch Loss: 0.2677\n",
      "  Batch Loss: 0.2282\n",
      "  Batch Loss: 0.1813\n",
      "  Batch Loss: 0.3093\n",
      "  Batch Loss: 0.4243\n",
      "  Batch Loss: 0.2621\n",
      "  Batch Loss: 0.2388\n",
      "  Batch Loss: 0.2183\n",
      "  Batch Loss: 0.3344\n",
      "  Batch Loss: 0.2266\n",
      "  Batch Loss: 0.1473\n",
      "  Batch Loss: 0.2383\n",
      "  Batch Loss: 0.1903\n",
      "  Batch Loss: 0.1168\n",
      "  Batch Loss: 0.2297\n",
      "  Batch Loss: 0.2445\n",
      "  Batch Loss: 0.2049\n",
      "  Batch Loss: 0.1867\n",
      "  Batch Loss: 0.1925\n",
      "  Batch Loss: 0.2916\n",
      "  Batch Loss: 0.1374\n",
      "  Batch Loss: 0.1602\n",
      "  Batch Loss: 0.2675\n",
      "  Batch Loss: 0.3665\n",
      "  Batch Loss: 0.2607\n",
      "  Batch Loss: 0.2990\n",
      "  Batch Loss: 0.2110\n",
      "  Batch Loss: 0.2676\n",
      "  Batch Loss: 0.2055\n",
      "  Batch Loss: 0.2985\n",
      "  Batch Loss: 0.2373\n",
      "  Batch Loss: 0.1445\n",
      "  Batch Loss: 0.2275\n",
      "  Batch Loss: 0.2927\n",
      "  Batch Loss: 0.2676\n",
      "  Batch Loss: 0.2494\n",
      "  Batch Loss: 0.2369\n",
      "  Batch Loss: 0.1998\n",
      "  Batch Loss: 0.2879\n",
      "  Batch Loss: 0.2263\n",
      "  Batch Loss: 0.2916\n",
      "  Batch Loss: 0.2578\n",
      "  Batch Loss: 0.1884\n",
      "  Batch Loss: 0.1927\n",
      "  Batch Loss: 0.2078\n",
      "  Batch Loss: 0.3107\n",
      "  Batch Loss: 0.1029\n",
      "  Batch Loss: 0.1903\n",
      "  Batch Loss: 0.2886\n",
      "  Batch Loss: 0.4169\n",
      "  Batch Loss: 0.1549\n",
      "  Batch Loss: 0.2594\n",
      "  Batch Loss: 0.3023\n",
      "  Batch Loss: 0.1559\n",
      "  Batch Loss: 0.2348\n",
      "  Batch Loss: 0.2206\n",
      "  Batch Loss: 0.2538\n",
      "  Batch Loss: 0.1941\n",
      "  Batch Loss: 0.2380\n",
      "  Batch Loss: 0.2243\n",
      "  Batch Loss: 0.2208\n",
      "  Batch Loss: 0.1638\n",
      "  Batch Loss: 0.3022\n",
      "  Batch Loss: 0.2139\n",
      "  Batch Loss: 0.1737\n",
      "  Batch Loss: 0.2285\n",
      "  Batch Loss: 0.1574\n",
      "  Batch Loss: 0.2563\n",
      "  Batch Loss: 0.1707\n",
      "  Batch Loss: 0.1973\n",
      "Average Loss: 0.2455\n",
      "Training complete. Model saved.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 148\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mRouge scores:\u001b[39m\u001b[39m'\u001b[39m, rouge_scores)\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[2], line 141\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining complete. Model saved.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[39m# Generate answers\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m generated_answers \u001b[39m=\u001b[39m generate_answers(questions, model, tokenizer)\n\u001b[1;32m    143\u001b[0m \u001b[39m# Evaluate Rouge scores\u001b[39;00m\n\u001b[1;32m    144\u001b[0m rouge_scores \u001b[39m=\u001b[39m evaluate_rouge(generated_answers, answers)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, AdamW\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "#gets data from csv\n",
    "def load_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    questions = data['Question'].tolist()\n",
    "    answers = data['Answer'].tolist()\n",
    "    return questions, answers\n",
    "\n",
    "#tokenises data and converts to integer representations of the token to be used as ids.\n",
    "def preprocess_data(questions, answers):\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for question in questions:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            question,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids.append(encoded.input_ids)\n",
    "        attention_masks.append(encoded.attention_mask)\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = tokenizer.batch_encode_plus(\n",
    "        answers,\n",
    "        add_special_tokens=True,\n",
    "        padding='max_length',\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    ).input_ids\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "#training function\n",
    "def train_model(input_ids, attention_masks, labels):\n",
    "    model = BartForConditionalGeneration.from_pretrained('textgenerator')\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_masks = attention_masks.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    model.train()\n",
    "    train_dataset = torch.utils.data.TensorDataset(input_ids, attention_masks, labels)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    #loops through dataset 5 times\n",
    "    for epoch in range(5):\n",
    "        total_loss = 0\n",
    "        print(f'Epoch: {epoch + 1}')\n",
    "        #batches of 8 qa pairs\n",
    "        for batch in train_loader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_masks, labels = batch\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_masks,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f'  Batch Loss: {loss.item():.4f}')\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#saves model\n",
    "def save_model(model, save_path):\n",
    "    model.save_pretrained(save_path)\n",
    "\n",
    "#generate answers\n",
    "def generate_answers(questions, model, tokenizer):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    generated_answers = []\n",
    "\n",
    "    for question in questions:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            question,\n",
    "            add_special_tokens=True,\n",
    "            padding='max_length',\n",
    "            max_length=256,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoded.input_ids.to(device)\n",
    "        attention_mask = encoded.attention_mask.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=256,\n",
    "                num_beams=5,\n",
    "                num_return_sequences=1,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_answers.append(generated_answer)\n",
    "\n",
    "    return generated_answers\n",
    "#rouge eval\n",
    "def evaluate_rouge(generated_answers, ground_truth_answers):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(generated_answers, ground_truth_answers)\n",
    "    return rouge_scores\n",
    "\n",
    "#main\n",
    "def main():\n",
    "    csv_file = 'qadata.csv'\n",
    "    save_path = 'textgenerator'\n",
    "    \n",
    "    questions, answers = load_data(csv_file)\n",
    "    input_ids, attention_masks, labels = preprocess_data(questions, answers)\n",
    "    model = train_model(input_ids, attention_masks, labels)\n",
    "    save_model(model, save_path)\n",
    "    print('Training complete. Model saved.')\n",
    "\n",
    "    #eval\n",
    "    tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "    generated_answers = generate_answers(questions, model, tokenizer)\n",
    "    rouge_scores = evaluate_rouge(generated_answers, answers)\n",
    "    print('Rouge scores:', rouge_scores)\n",
    "\n",
    "#callable from other script\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge scores: [{'rouge1': Score(precision=0.27906976744186046, recall=0.375, fmeasure=0.31999999999999995), 'rouge2': Score(precision=0.047619047619047616, recall=0.06451612903225806, fmeasure=0.0547945205479452), 'rougeL': Score(precision=0.20930232558139536, recall=0.28125, fmeasure=0.24)}, {'rouge1': Score(precision=0.803921568627451, recall=0.8913043478260869, fmeasure=0.845360824742268), 'rouge2': Score(precision=0.72, recall=0.8, fmeasure=0.7578947368421052), 'rougeL': Score(precision=0.7647058823529411, recall=0.8478260869565217, fmeasure=0.8041237113402062)}, {'rouge1': Score(precision=0.5238095238095238, recall=0.7333333333333333, fmeasure=0.611111111111111), 'rouge2': Score(precision=0.34146341463414637, recall=0.4827586206896552, fmeasure=0.4000000000000001), 'rougeL': Score(precision=0.35714285714285715, recall=0.5, fmeasure=0.41666666666666663)}, {'rouge1': Score(precision=0.43137254901960786, recall=0.4888888888888889, fmeasure=0.45833333333333326), 'rouge2': Score(precision=0.16, recall=0.18181818181818182, fmeasure=0.1702127659574468), 'rougeL': Score(precision=0.21568627450980393, recall=0.24444444444444444, fmeasure=0.22916666666666663)}, {'rouge1': Score(precision=0.4117647058823529, recall=0.35, fmeasure=0.37837837837837834), 'rouge2': Score(precision=0.26, recall=0.22033898305084745, fmeasure=0.23853211009174313), 'rougeL': Score(precision=0.3137254901960784, recall=0.26666666666666666, fmeasure=0.2882882882882883)}, {'rouge1': Score(precision=0.3893805309734513, recall=0.9166666666666666, fmeasure=0.5465838509316769), 'rouge2': Score(precision=0.35714285714285715, recall=0.851063829787234, fmeasure=0.5031446540880503), 'rougeL': Score(precision=0.3805309734513274, recall=0.8958333333333334, fmeasure=0.5341614906832298)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.8611111111111112, recall=0.8378378378378378, fmeasure=0.8493150684931507), 'rouge2': Score(precision=0.7714285714285715, recall=0.75, fmeasure=0.7605633802816902), 'rougeL': Score(precision=0.8611111111111112, recall=0.8378378378378378, fmeasure=0.8493150684931507)}, {'rouge1': Score(precision=0.3924050632911392, recall=0.5636363636363636, fmeasure=0.46268656716417905), 'rouge2': Score(precision=0.2564102564102564, recall=0.37037037037037035, fmeasure=0.30303030303030304), 'rougeL': Score(precision=0.34177215189873417, recall=0.4909090909090909, fmeasure=0.4029850746268656)}, {'rouge1': Score(precision=0.9298245614035088, recall=1.0, fmeasure=0.9636363636363636), 'rouge2': Score(precision=0.8392857142857143, recall=0.9038461538461539, fmeasure=0.8703703703703703), 'rougeL': Score(precision=0.9122807017543859, recall=0.9811320754716981, fmeasure=0.9454545454545454)}, {'rouge1': Score(precision=0.717948717948718, recall=0.7777777777777778, fmeasure=0.7466666666666666), 'rouge2': Score(precision=0.5194805194805194, recall=0.5633802816901409, fmeasure=0.5405405405405406), 'rougeL': Score(precision=0.6538461538461539, recall=0.7083333333333334, fmeasure=0.68)}, {'rouge1': Score(precision=0.4857142857142857, recall=0.68, fmeasure=0.5666666666666667), 'rouge2': Score(precision=0.36538461538461536, recall=0.5135135135135135, fmeasure=0.42696629213483145), 'rougeL': Score(precision=0.41904761904761906, recall=0.5866666666666667, fmeasure=0.4888888888888889)}, {'rouge1': Score(precision=0.5408163265306123, recall=1.0, fmeasure=0.7019867549668874), 'rouge2': Score(precision=0.5154639175257731, recall=0.9615384615384616, fmeasure=0.6711409395973154), 'rougeL': Score(precision=0.5408163265306123, recall=1.0, fmeasure=0.7019867549668874)}, {'rouge1': Score(precision=0.5652173913043478, recall=0.52, fmeasure=0.5416666666666667), 'rouge2': Score(precision=0.3181818181818182, recall=0.2916666666666667, fmeasure=0.30434782608695654), 'rougeL': Score(precision=0.43478260869565216, recall=0.4, fmeasure=0.41666666666666663)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.3619047619047619, recall=0.8636363636363636, fmeasure=0.5100671140939597), 'rouge2': Score(precision=0.23076923076923078, recall=0.5581395348837209, fmeasure=0.326530612244898), 'rougeL': Score(precision=0.24761904761904763, recall=0.5909090909090909, fmeasure=0.348993288590604)}, {'rouge1': Score(precision=0.9583333333333334, recall=1.0, fmeasure=0.9787234042553191), 'rouge2': Score(precision=0.9574468085106383, recall=1.0, fmeasure=0.9782608695652174), 'rougeL': Score(precision=0.9583333333333334, recall=1.0, fmeasure=0.9787234042553191)}, {'rouge1': Score(precision=0.6666666666666666, recall=0.85, fmeasure=0.7472527472527473), 'rouge2': Score(precision=0.58, recall=0.7435897435897436, fmeasure=0.6516853932584269), 'rougeL': Score(precision=0.6470588235294118, recall=0.825, fmeasure=0.7252747252747253)}, {'rouge1': Score(precision=0.9655172413793104, recall=0.875, fmeasure=0.9180327868852458), 'rouge2': Score(precision=0.8928571428571429, recall=0.8064516129032258, fmeasure=0.8474576271186439), 'rougeL': Score(precision=0.8620689655172413, recall=0.78125, fmeasure=0.8196721311475409)}, {'rouge1': Score(precision=0.5409836065573771, recall=0.7857142857142857, fmeasure=0.6407766990291263), 'rouge2': Score(precision=0.45, recall=0.6585365853658537, fmeasure=0.5346534653465347), 'rougeL': Score(precision=0.5081967213114754, recall=0.7380952380952381, fmeasure=0.6019417475728156)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=0.7777777777777778, recall=0.7777777777777778, fmeasure=0.7777777777777778), 'rougeL': Score(precision=0.9, recall=0.9, fmeasure=0.9)}, {'rouge1': Score(precision=0.7272727272727273, recall=0.6153846153846154, fmeasure=0.6666666666666667), 'rouge2': Score(precision=0.6, recall=0.5, fmeasure=0.5454545454545454), 'rougeL': Score(precision=0.7272727272727273, recall=0.6153846153846154, fmeasure=0.6666666666666667)}, {'rouge1': Score(precision=0.926829268292683, recall=0.926829268292683, fmeasure=0.926829268292683), 'rouge2': Score(precision=0.85, recall=0.85, fmeasure=0.85), 'rougeL': Score(precision=0.926829268292683, recall=0.926829268292683, fmeasure=0.926829268292683)}, {'rouge1': Score(precision=0.7619047619047619, recall=0.6153846153846154, fmeasure=0.6808510638297872), 'rouge2': Score(precision=0.4, recall=0.32, fmeasure=0.35555555555555557), 'rougeL': Score(precision=0.42857142857142855, recall=0.34615384615384615, fmeasure=0.3829787234042554)}, {'rouge1': Score(precision=0.8, recall=0.96, fmeasure=0.8727272727272728), 'rouge2': Score(precision=0.6896551724137931, recall=0.8333333333333334, fmeasure=0.7547169811320755), 'rougeL': Score(precision=0.5666666666666667, recall=0.68, fmeasure=0.6181818181818183)}, {'rouge1': Score(precision=0.9655172413793104, recall=0.9655172413793104, fmeasure=0.9655172413793104), 'rouge2': Score(precision=0.9285714285714286, recall=0.9285714285714286, fmeasure=0.9285714285714286), 'rougeL': Score(precision=0.9655172413793104, recall=0.9655172413793104, fmeasure=0.9655172413793104)}, {'rouge1': Score(precision=0.09345794392523364, recall=0.9090909090909091, fmeasure=0.16949152542372883), 'rouge2': Score(precision=0.07547169811320754, recall=0.8, fmeasure=0.13793103448275862), 'rougeL': Score(precision=0.07476635514018691, recall=0.7272727272727273, fmeasure=0.13559322033898302)}, {'rouge1': Score(precision=0.47058823529411764, recall=0.6666666666666666, fmeasure=0.5517241379310345), 'rouge2': Score(precision=0.125, recall=0.18181818181818182, fmeasure=0.14814814814814814), 'rougeL': Score(precision=0.29411764705882354, recall=0.4166666666666667, fmeasure=0.3448275862068966)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.9310344827586207, recall=1.0, fmeasure=0.9642857142857143), 'rouge2': Score(precision=0.9285714285714286, recall=1.0, fmeasure=0.962962962962963), 'rougeL': Score(precision=0.9310344827586207, recall=1.0, fmeasure=0.9642857142857143)}, {'rouge1': Score(precision=0.42424242424242425, recall=0.8235294117647058, fmeasure=0.5599999999999999), 'rouge2': Score(precision=0.2923076923076923, recall=0.5757575757575758, fmeasure=0.3877551020408164), 'rougeL': Score(precision=0.21212121212121213, recall=0.4117647058823529, fmeasure=0.27999999999999997)}, {'rouge1': Score(precision=0.55, recall=0.9705882352941176, fmeasure=0.7021276595744681), 'rouge2': Score(precision=0.5254237288135594, recall=0.9393939393939394, fmeasure=0.673913043478261), 'rougeL': Score(precision=0.55, recall=0.9705882352941176, fmeasure=0.7021276595744681)}, {'rouge1': Score(precision=0.8888888888888888, recall=0.8888888888888888, fmeasure=0.8888888888888888), 'rouge2': Score(precision=0.7692307692307693, recall=0.7692307692307693, fmeasure=0.7692307692307693), 'rougeL': Score(precision=0.8888888888888888, recall=0.8888888888888888, fmeasure=0.8888888888888888)}, {'rouge1': Score(precision=0.6363636363636364, recall=0.28, fmeasure=0.3888888888888889), 'rouge2': Score(precision=0.4, recall=0.16666666666666666, fmeasure=0.23529411764705882), 'rougeL': Score(precision=0.36363636363636365, recall=0.16, fmeasure=0.2222222222222222)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.8444444444444444, recall=0.9047619047619048, fmeasure=0.8735632183908046), 'rouge2': Score(precision=0.7045454545454546, recall=0.7560975609756098, fmeasure=0.7294117647058823), 'rougeL': Score(precision=0.8444444444444444, recall=0.9047619047619048, fmeasure=0.8735632183908046)}, {'rouge1': Score(precision=0.12698412698412698, recall=0.22857142857142856, fmeasure=0.163265306122449), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.07936507936507936, recall=0.14285714285714285, fmeasure=0.1020408163265306)}, {'rouge1': Score(precision=0.05084745762711865, recall=0.08108108108108109, fmeasure=0.0625), 'rouge2': Score(precision=0.0, recall=0.0, fmeasure=0.0), 'rougeL': Score(precision=0.03389830508474576, recall=0.05405405405405406, fmeasure=0.04166666666666667)}, {'rouge1': Score(precision=0.6956521739130435, recall=0.7111111111111111, fmeasure=0.7032967032967032), 'rouge2': Score(precision=0.4444444444444444, recall=0.45454545454545453, fmeasure=0.44943820224719094), 'rougeL': Score(precision=0.6521739130434783, recall=0.6666666666666666, fmeasure=0.6593406593406593)}, {'rouge1': Score(precision=0.8947368421052632, recall=0.9444444444444444, fmeasure=0.918918918918919), 'rouge2': Score(precision=0.8333333333333334, recall=0.8823529411764706, fmeasure=0.8571428571428571), 'rougeL': Score(precision=0.8947368421052632, recall=0.9444444444444444, fmeasure=0.918918918918919)}, {'rouge1': Score(precision=0.30158730158730157, recall=0.6129032258064516, fmeasure=0.40425531914893614), 'rouge2': Score(precision=0.12903225806451613, recall=0.26666666666666666, fmeasure=0.17391304347826086), 'rougeL': Score(precision=0.2222222222222222, recall=0.45161290322580644, fmeasure=0.2978723404255319)}, {'rouge1': Score(precision=0.20567375886524822, recall=0.7837837837837838, fmeasure=0.3258426966292135), 'rouge2': Score(precision=0.11428571428571428, recall=0.4444444444444444, fmeasure=0.1818181818181818), 'rougeL': Score(precision=0.12056737588652482, recall=0.4594594594594595, fmeasure=0.19101123595505617)}, {'rouge1': Score(precision=0.40625, recall=0.8387096774193549, fmeasure=0.5473684210526316), 'rouge2': Score(precision=0.3333333333333333, recall=0.7, fmeasure=0.45161290322580644), 'rougeL': Score(precision=0.40625, recall=0.8387096774193549, fmeasure=0.5473684210526316)}, {'rouge1': Score(precision=0.6119402985074627, recall=0.7192982456140351, fmeasure=0.6612903225806452), 'rouge2': Score(precision=0.4393939393939394, recall=0.5178571428571429, fmeasure=0.47540983606557385), 'rougeL': Score(precision=0.582089552238806, recall=0.6842105263157895, fmeasure=0.6290322580645161)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.64, recall=0.6666666666666666, fmeasure=0.6530612244897959), 'rouge2': Score(precision=0.375, recall=0.391304347826087, fmeasure=0.3829787234042554), 'rougeL': Score(precision=0.56, recall=0.5833333333333334, fmeasure=0.5714285714285714)}, {'rouge1': Score(precision=0.7419354838709677, recall=0.8679245283018868, fmeasure=0.8), 'rouge2': Score(precision=0.6065573770491803, recall=0.7115384615384616, fmeasure=0.654867256637168), 'rougeL': Score(precision=0.7258064516129032, recall=0.8490566037735849, fmeasure=0.782608695652174)}, {'rouge1': Score(precision=0.7321428571428571, recall=0.9111111111111111, fmeasure=0.8118811881188118), 'rouge2': Score(precision=0.6545454545454545, recall=0.8181818181818182, fmeasure=0.7272727272727274), 'rougeL': Score(precision=0.6964285714285714, recall=0.8666666666666667, fmeasure=0.7722772277227722)}, {'rouge1': Score(precision=0.8524590163934426, recall=0.8666666666666667, fmeasure=0.8595041322314049), 'rouge2': Score(precision=0.75, recall=0.7627118644067796, fmeasure=0.7563025210084034), 'rougeL': Score(precision=0.8524590163934426, recall=0.8666666666666667, fmeasure=0.8595041322314049)}, {'rouge1': Score(precision=0.609375, recall=0.8863636363636364, fmeasure=0.7222222222222223), 'rouge2': Score(precision=0.5555555555555556, recall=0.813953488372093, fmeasure=0.6603773584905661), 'rougeL': Score(precision=0.40625, recall=0.5909090909090909, fmeasure=0.4814814814814815)}, {'rouge1': Score(precision=0.25, recall=0.47619047619047616, fmeasure=0.32786885245901637), 'rouge2': Score(precision=0.012658227848101266, recall=0.024390243902439025, fmeasure=0.016666666666666666), 'rougeL': Score(precision=0.1125, recall=0.21428571428571427, fmeasure=0.14754098360655735)}, {'rouge1': Score(precision=0.6101694915254238, recall=0.7058823529411765, fmeasure=0.6545454545454547), 'rouge2': Score(precision=0.41379310344827586, recall=0.48, fmeasure=0.4444444444444445), 'rougeL': Score(precision=0.559322033898305, recall=0.6470588235294118, fmeasure=0.6)}, {'rouge1': Score(precision=0.5476190476190477, recall=0.7540983606557377, fmeasure=0.6344827586206897), 'rouge2': Score(precision=0.46987951807228917, recall=0.65, fmeasure=0.5454545454545455), 'rougeL': Score(precision=0.5238095238095238, recall=0.7213114754098361, fmeasure=0.6068965517241379)}, {'rouge1': Score(precision=0.6082474226804123, recall=0.9672131147540983, fmeasure=0.7468354430379747), 'rouge2': Score(precision=0.5416666666666666, recall=0.8666666666666667, fmeasure=0.6666666666666667), 'rougeL': Score(precision=0.5773195876288659, recall=0.9180327868852459, fmeasure=0.7088607594936709)}, {'rouge1': Score(precision=0.9310344827586207, recall=0.9310344827586207, fmeasure=0.9310344827586207), 'rouge2': Score(precision=0.8928571428571429, recall=0.8928571428571429, fmeasure=0.8928571428571429), 'rougeL': Score(precision=0.9310344827586207, recall=0.9310344827586207, fmeasure=0.9310344827586207)}, {'rouge1': Score(precision=0.96875, recall=0.96875, fmeasure=0.96875), 'rouge2': Score(precision=0.9354838709677419, recall=0.9354838709677419, fmeasure=0.9354838709677419), 'rougeL': Score(precision=0.96875, recall=0.96875, fmeasure=0.96875)}, {'rouge1': Score(precision=0.5319148936170213, recall=1.0, fmeasure=0.6944444444444444), 'rouge2': Score(precision=0.5161290322580645, recall=0.9795918367346939, fmeasure=0.6760563380281689), 'rougeL': Score(precision=0.5106382978723404, recall=0.96, fmeasure=0.6666666666666666)}, {'rouge1': Score(precision=0.057692307692307696, recall=0.8571428571428571, fmeasure=0.1081081081081081), 'rouge2': Score(precision=0.038834951456310676, recall=0.6666666666666666, fmeasure=0.07339449541284404), 'rougeL': Score(precision=0.057692307692307696, recall=0.8571428571428571, fmeasure=0.1081081081081081)}, {'rouge1': Score(precision=0.36, recall=0.375, fmeasure=0.3673469387755102), 'rouge2': Score(precision=0.22448979591836735, recall=0.23404255319148937, fmeasure=0.22916666666666666), 'rougeL': Score(precision=0.32, recall=0.3333333333333333, fmeasure=0.32653061224489793)}, {'rouge1': Score(precision=0.26956521739130435, recall=0.4626865671641791, fmeasure=0.34065934065934067), 'rouge2': Score(precision=0.13157894736842105, recall=0.22727272727272727, fmeasure=0.16666666666666666), 'rougeL': Score(precision=0.19130434782608696, recall=0.3283582089552239, fmeasure=0.2417582417582417)}, {'rouge1': Score(precision=0.42857142857142855, recall=0.42857142857142855, fmeasure=0.42857142857142855), 'rouge2': Score(precision=0.16666666666666666, recall=0.16666666666666666, fmeasure=0.16666666666666666), 'rougeL': Score(precision=0.42857142857142855, recall=0.42857142857142855, fmeasure=0.42857142857142855)}, {'rouge1': Score(precision=0.9642857142857143, recall=0.9473684210526315, fmeasure=0.9557522123893805), 'rouge2': Score(precision=0.9636363636363636, recall=0.9464285714285714, fmeasure=0.9549549549549549), 'rougeL': Score(precision=0.9642857142857143, recall=0.9473684210526315, fmeasure=0.9557522123893805)}, {'rouge1': Score(precision=0.8461538461538461, recall=0.88, fmeasure=0.8627450980392156), 'rouge2': Score(precision=0.72, recall=0.75, fmeasure=0.7346938775510204), 'rougeL': Score(precision=0.8461538461538461, recall=0.88, fmeasure=0.8627450980392156)}, {'rouge1': Score(precision=0.8533333333333334, recall=1.0, fmeasure=0.9208633093525179), 'rouge2': Score(precision=0.8513513513513513, recall=1.0, fmeasure=0.9197080291970803), 'rougeL': Score(precision=0.8533333333333334, recall=1.0, fmeasure=0.9208633093525179)}, {'rouge1': Score(precision=0.36363636363636365, recall=0.4, fmeasure=0.380952380952381), 'rouge2': Score(precision=0.015384615384615385, recall=0.01694915254237288, fmeasure=0.016129032258064516), 'rougeL': Score(precision=0.18181818181818182, recall=0.2, fmeasure=0.1904761904761905)}, {'rouge1': Score(precision=0.15517241379310345, recall=0.6, fmeasure=0.24657534246575347), 'rouge2': Score(precision=0.0782608695652174, recall=0.3103448275862069, fmeasure=0.12500000000000003), 'rougeL': Score(precision=0.1206896551724138, recall=0.4666666666666667, fmeasure=0.19178082191780824)}, {'rouge1': Score(precision=0.5970149253731343, recall=0.7272727272727273, fmeasure=0.6557377049180327), 'rouge2': Score(precision=0.3787878787878788, recall=0.46296296296296297, fmeasure=0.4166666666666667), 'rougeL': Score(precision=0.5522388059701493, recall=0.6727272727272727, fmeasure=0.6065573770491803)}, {'rouge1': Score(precision=0.18705035971223022, recall=0.5909090909090909, fmeasure=0.28415300546448086), 'rouge2': Score(precision=0.028985507246376812, recall=0.09302325581395349, fmeasure=0.04419889502762431), 'rougeL': Score(precision=0.08633093525179857, recall=0.2727272727272727, fmeasure=0.13114754098360656)}, {'rouge1': Score(precision=0.3395061728395062, recall=0.859375, fmeasure=0.48672566371681414), 'rouge2': Score(precision=0.19254658385093168, recall=0.49206349206349204, fmeasure=0.2767857142857143), 'rougeL': Score(precision=0.2037037037037037, recall=0.515625, fmeasure=0.29203539823008845)}, {'rouge1': Score(precision=0.21705426356589147, recall=0.875, fmeasure=0.3478260869565218), 'rouge2': Score(precision=0.1484375, recall=0.6129032258064516, fmeasure=0.23899371069182387), 'rougeL': Score(precision=0.16279069767441862, recall=0.65625, fmeasure=0.2608695652173913)}, {'rouge1': Score(precision=0.7735849056603774, recall=0.9318181818181818, fmeasure=0.8453608247422681), 'rouge2': Score(precision=0.7115384615384616, recall=0.8604651162790697, fmeasure=0.7789473684210527), 'rougeL': Score(precision=0.7735849056603774, recall=0.9318181818181818, fmeasure=0.8453608247422681)}, {'rouge1': Score(precision=0.8181818181818182, recall=1.0, fmeasure=0.9), 'rouge2': Score(precision=0.7962962962962963, recall=0.9772727272727273, fmeasure=0.8775510204081632), 'rougeL': Score(precision=0.8181818181818182, recall=1.0, fmeasure=0.9)}, {'rouge1': Score(precision=0.5581395348837209, recall=0.7741935483870968, fmeasure=0.6486486486486487), 'rouge2': Score(precision=0.3333333333333333, recall=0.4666666666666667, fmeasure=0.3888888888888889), 'rougeL': Score(precision=0.5116279069767442, recall=0.7096774193548387, fmeasure=0.5945945945945946)}, {'rouge1': Score(precision=0.5757575757575758, recall=0.8837209302325582, fmeasure=0.6972477064220183), 'rouge2': Score(precision=0.4, recall=0.6190476190476191, fmeasure=0.485981308411215), 'rougeL': Score(precision=0.36363636363636365, recall=0.5581395348837209, fmeasure=0.4403669724770642)}, {'rouge1': Score(precision=0.47619047619047616, recall=0.8823529411764706, fmeasure=0.6185567010309277), 'rouge2': Score(precision=0.41935483870967744, recall=0.7878787878787878, fmeasure=0.5473684210526315), 'rougeL': Score(precision=0.47619047619047616, recall=0.8823529411764706, fmeasure=0.6185567010309277)}, {'rouge1': Score(precision=0.9, recall=0.8780487804878049, fmeasure=0.888888888888889), 'rouge2': Score(precision=0.7948717948717948, recall=0.775, fmeasure=0.7848101265822786), 'rougeL': Score(precision=0.75, recall=0.7317073170731707, fmeasure=0.7407407407407408)}, {'rouge1': Score(precision=0.927710843373494, recall=0.9746835443037974, fmeasure=0.9506172839506173), 'rouge2': Score(precision=0.9146341463414634, recall=0.9615384615384616, fmeasure=0.9375000000000001), 'rougeL': Score(precision=0.927710843373494, recall=0.9746835443037974, fmeasure=0.9506172839506173)}, {'rouge1': Score(precision=0.3764705882352941, recall=0.9696969696969697, fmeasure=0.5423728813559322), 'rouge2': Score(precision=0.3333333333333333, recall=0.875, fmeasure=0.48275862068965514), 'rougeL': Score(precision=0.3764705882352941, recall=0.9696969696969697, fmeasure=0.5423728813559322)}, {'rouge1': Score(precision=0.2556390977443609, recall=0.576271186440678, fmeasure=0.35416666666666663), 'rouge2': Score(precision=0.10606060606060606, recall=0.2413793103448276, fmeasure=0.1473684210526316), 'rougeL': Score(precision=0.16541353383458646, recall=0.3728813559322034, fmeasure=0.22916666666666666)}, {'rouge1': Score(precision=0.5857142857142857, recall=1.0, fmeasure=0.7387387387387387), 'rouge2': Score(precision=0.5797101449275363, recall=1.0, fmeasure=0.7339449541284404), 'rougeL': Score(precision=0.5857142857142857, recall=1.0, fmeasure=0.7387387387387387)}, {'rouge1': Score(precision=0.5, recall=0.4883720930232558, fmeasure=0.4941176470588235), 'rouge2': Score(precision=0.34146341463414637, recall=0.3333333333333333, fmeasure=0.3373493975903615), 'rougeL': Score(precision=0.40476190476190477, recall=0.3953488372093023, fmeasure=0.4)}, {'rouge1': Score(precision=0.49523809523809526, recall=0.8813559322033898, fmeasure=0.6341463414634146), 'rouge2': Score(precision=0.3942307692307692, recall=0.7068965517241379, fmeasure=0.5061728395061729), 'rougeL': Score(precision=0.47619047619047616, recall=0.847457627118644, fmeasure=0.6097560975609755)}, {'rouge1': Score(precision=0.4857142857142857, recall=0.6538461538461539, fmeasure=0.5573770491803278), 'rouge2': Score(precision=0.29411764705882354, recall=0.4, fmeasure=0.33898305084745767), 'rougeL': Score(precision=0.45714285714285713, recall=0.6153846153846154, fmeasure=0.5245901639344263)}, {'rouge1': Score(precision=0.6, recall=0.8, fmeasure=0.6857142857142857), 'rouge2': Score(precision=0.4406779661016949, recall=0.5909090909090909, fmeasure=0.5048543689320388), 'rougeL': Score(precision=0.5666666666666667, recall=0.7555555555555555, fmeasure=0.6476190476190475)}, {'rouge1': Score(precision=0.5490196078431373, recall=0.7777777777777778, fmeasure=0.6436781609195402), 'rouge2': Score(precision=0.3, recall=0.42857142857142855, fmeasure=0.3529411764705882), 'rougeL': Score(precision=0.47058823529411764, recall=0.6666666666666666, fmeasure=0.5517241379310345)}, {'rouge1': Score(precision=0.6, recall=0.8, fmeasure=0.6857142857142857), 'rouge2': Score(precision=0.4406779661016949, recall=0.5909090909090909, fmeasure=0.5048543689320388), 'rougeL': Score(precision=0.5666666666666667, recall=0.7555555555555555, fmeasure=0.6476190476190475)}, {'rouge1': Score(precision=0.7857142857142857, recall=0.8461538461538461, fmeasure=0.8148148148148148), 'rouge2': Score(precision=0.6341463414634146, recall=0.6842105263157895, fmeasure=0.6582278481012659), 'rougeL': Score(precision=0.7857142857142857, recall=0.8461538461538461, fmeasure=0.8148148148148148)}, {'rouge1': Score(precision=0.9705882352941176, recall=0.9705882352941176, fmeasure=0.9705882352941176), 'rouge2': Score(precision=0.9393939393939394, recall=0.9393939393939394, fmeasure=0.9393939393939394), 'rougeL': Score(precision=0.9705882352941176, recall=0.9705882352941176, fmeasure=0.9705882352941176)}, {'rouge1': Score(precision=0.2890625, recall=0.6491228070175439, fmeasure=0.4), 'rouge2': Score(precision=0.11023622047244094, recall=0.25, fmeasure=0.1530054644808743), 'rougeL': Score(precision=0.1953125, recall=0.43859649122807015, fmeasure=0.27027027027027023)}, {'rouge1': Score(precision=0.6483516483516484, recall=0.8939393939393939, fmeasure=0.751592356687898), 'rouge2': Score(precision=0.5444444444444444, recall=0.7538461538461538, fmeasure=0.6322580645161291), 'rougeL': Score(precision=0.5054945054945055, recall=0.696969696969697, fmeasure=0.5859872611464968)}, {'rouge1': Score(precision=0.9230769230769231, recall=0.9411764705882353, fmeasure=0.9320388349514563), 'rouge2': Score(precision=0.8627450980392157, recall=0.88, fmeasure=0.8712871287128714), 'rougeL': Score(precision=0.9230769230769231, recall=0.9411764705882353, fmeasure=0.9320388349514563)}, {'rouge1': Score(precision=0.3787878787878788, recall=0.3472222222222222, fmeasure=0.36231884057971014), 'rouge2': Score(precision=0.046153846153846156, recall=0.04225352112676056, fmeasure=0.04411764705882353), 'rougeL': Score(precision=0.21212121212121213, recall=0.19444444444444445, fmeasure=0.20289855072463767)}, {'rouge1': Score(precision=0.6666666666666666, recall=0.6086956521739131, fmeasure=0.6363636363636365), 'rouge2': Score(precision=0.5, recall=0.45588235294117646, fmeasure=0.47692307692307695), 'rougeL': Score(precision=0.6349206349206349, recall=0.5797101449275363, fmeasure=0.6060606060606061)}, {'rouge1': Score(precision=0.8405797101449275, recall=0.9830508474576272, fmeasure=0.9062499999999999), 'rouge2': Score(precision=0.7647058823529411, recall=0.896551724137931, fmeasure=0.8253968253968255), 'rougeL': Score(precision=0.8260869565217391, recall=0.9661016949152542, fmeasure=0.890625)}, {'rouge1': Score(precision=0.2967032967032967, recall=0.5510204081632653, fmeasure=0.3857142857142857), 'rouge2': Score(precision=0.15555555555555556, recall=0.2916666666666667, fmeasure=0.2028985507246377), 'rougeL': Score(precision=0.23076923076923078, recall=0.42857142857142855, fmeasure=0.3)}, {'rouge1': Score(precision=0.32786885245901637, recall=0.8888888888888888, fmeasure=0.47904191616766467), 'rouge2': Score(precision=0.2892561983471074, recall=0.7954545454545454, fmeasure=0.42424242424242425), 'rougeL': Score(precision=0.319672131147541, recall=0.8666666666666667, fmeasure=0.4670658682634731)}, {'rouge1': Score(precision=0.8541666666666666, recall=0.8541666666666666, fmeasure=0.8541666666666666), 'rouge2': Score(precision=0.7446808510638298, recall=0.7446808510638298, fmeasure=0.7446808510638298), 'rougeL': Score(precision=0.8541666666666666, recall=0.8541666666666666, fmeasure=0.8541666666666666)}, {'rouge1': Score(precision=0.5283018867924528, recall=0.6363636363636364, fmeasure=0.5773195876288659), 'rouge2': Score(precision=0.3269230769230769, recall=0.3953488372093023, fmeasure=0.3578947368421052), 'rougeL': Score(precision=0.3584905660377358, recall=0.4318181818181818, fmeasure=0.39175257731958757)}, {'rouge1': Score(precision=0.3018867924528302, recall=0.5614035087719298, fmeasure=0.39263803680981596), 'rouge2': Score(precision=0.19047619047619047, recall=0.35714285714285715, fmeasure=0.24844720496894404), 'rougeL': Score(precision=0.2169811320754717, recall=0.40350877192982454, fmeasure=0.28220858895705525)}, {'rouge1': Score(precision=0.4791666666666667, recall=0.7301587301587301, fmeasure=0.578616352201258), 'rouge2': Score(precision=0.4, recall=0.6129032258064516, fmeasure=0.4840764331210191), 'rougeL': Score(precision=0.4375, recall=0.6666666666666666, fmeasure=0.5283018867924528)}, {'rouge1': Score(precision=0.28, recall=1.0, fmeasure=0.43750000000000006), 'rouge2': Score(precision=0.2702702702702703, recall=1.0, fmeasure=0.4255319148936171), 'rougeL': Score(precision=0.28, recall=1.0, fmeasure=0.43750000000000006)}, {'rouge1': Score(precision=0.5952380952380952, recall=0.6578947368421053, fmeasure=0.625), 'rouge2': Score(precision=0.43902439024390244, recall=0.4864864864864865, fmeasure=0.4615384615384615), 'rougeL': Score(precision=0.5714285714285714, recall=0.631578947368421, fmeasure=0.6)}, {'rouge1': Score(precision=0.4936708860759494, recall=0.8863636363636364, fmeasure=0.6341463414634146), 'rouge2': Score(precision=0.3333333333333333, recall=0.6046511627906976, fmeasure=0.4297520661157025), 'rougeL': Score(precision=0.46835443037974683, recall=0.8409090909090909, fmeasure=0.6016260162601627)}, {'rouge1': Score(precision=0.6774193548387096, recall=0.5833333333333334, fmeasure=0.626865671641791), 'rouge2': Score(precision=0.5333333333333333, recall=0.45714285714285713, fmeasure=0.4923076923076923), 'rougeL': Score(precision=0.5806451612903226, recall=0.5, fmeasure=0.537313432835821)}, {'rouge1': Score(precision=0.37037037037037035, recall=0.75, fmeasure=0.4958677685950414), 'rouge2': Score(precision=0.2625, recall=0.5384615384615384, fmeasure=0.3529411764705882), 'rougeL': Score(precision=0.32098765432098764, recall=0.65, fmeasure=0.4297520661157025)}, {'rouge1': Score(precision=0.9180327868852459, recall=0.875, fmeasure=0.8959999999999999), 'rouge2': Score(precision=0.8166666666666667, recall=0.7777777777777778, fmeasure=0.7967479674796747), 'rougeL': Score(precision=0.9016393442622951, recall=0.859375, fmeasure=0.88)}, {'rouge1': Score(precision=0.53125, recall=0.6938775510204082, fmeasure=0.6017699115044248), 'rouge2': Score(precision=0.38095238095238093, recall=0.5, fmeasure=0.4324324324324324), 'rougeL': Score(precision=0.484375, recall=0.6326530612244898, fmeasure=0.5486725663716815)}, {'rouge1': Score(precision=0.7543859649122807, recall=0.8431372549019608, fmeasure=0.7962962962962964), 'rouge2': Score(precision=0.7321428571428571, recall=0.82, fmeasure=0.7735849056603773), 'rougeL': Score(precision=0.7543859649122807, recall=0.8431372549019608, fmeasure=0.7962962962962964)}, {'rouge1': Score(precision=0.49206349206349204, recall=0.9117647058823529, fmeasure=0.6391752577319587), 'rouge2': Score(precision=0.41935483870967744, recall=0.7878787878787878, fmeasure=0.5473684210526315), 'rougeL': Score(precision=0.47619047619047616, recall=0.8823529411764706, fmeasure=0.6185567010309277)}, {'rouge1': Score(precision=0.75, recall=0.9782608695652174, fmeasure=0.849056603773585), 'rouge2': Score(precision=0.6949152542372882, recall=0.9111111111111111, fmeasure=0.7884615384615385), 'rougeL': Score(precision=0.7333333333333333, recall=0.9565217391304348, fmeasure=0.8301886792452831)}, {'rouge1': Score(precision=0.6222222222222222, recall=0.717948717948718, fmeasure=0.6666666666666666), 'rouge2': Score(precision=0.4772727272727273, recall=0.5526315789473685, fmeasure=0.5121951219512196), 'rougeL': Score(precision=0.6222222222222222, recall=0.717948717948718, fmeasure=0.6666666666666666)}, {'rouge1': Score(precision=0.5, recall=0.5806451612903226, fmeasure=0.537313432835821), 'rouge2': Score(precision=0.34285714285714286, recall=0.4, fmeasure=0.36923076923076925), 'rougeL': Score(precision=0.4444444444444444, recall=0.5161290322580645, fmeasure=0.4776119402985074)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.6981132075471698, recall=0.9487179487179487, fmeasure=0.8043478260869565), 'rouge2': Score(precision=0.5576923076923077, recall=0.7631578947368421, fmeasure=0.6444444444444444), 'rougeL': Score(precision=0.660377358490566, recall=0.8974358974358975, fmeasure=0.7608695652173912)}, {'rouge1': Score(precision=0.527027027027027, recall=0.75, fmeasure=0.619047619047619), 'rouge2': Score(precision=0.3561643835616438, recall=0.5098039215686274, fmeasure=0.4193548387096774), 'rougeL': Score(precision=0.5, recall=0.7115384615384616, fmeasure=0.5873015873015872)}, {'rouge1': Score(precision=0.8666666666666667, recall=0.7647058823529411, fmeasure=0.8125), 'rouge2': Score(precision=0.8571428571428571, recall=0.75, fmeasure=0.7999999999999999), 'rougeL': Score(precision=0.8666666666666667, recall=0.7647058823529411, fmeasure=0.8125)}, {'rouge1': Score(precision=0.2909090909090909, recall=0.5333333333333333, fmeasure=0.3764705882352941), 'rouge2': Score(precision=0.14814814814814814, recall=0.27586206896551724, fmeasure=0.19277108433734938), 'rougeL': Score(precision=0.23636363636363636, recall=0.43333333333333335, fmeasure=0.3058823529411765)}, {'rouge1': Score(precision=0.5, recall=1.0, fmeasure=0.6666666666666666), 'rouge2': Score(precision=0.48148148148148145, recall=1.0, fmeasure=0.65), 'rougeL': Score(precision=0.5, recall=1.0, fmeasure=0.6666666666666666)}, {'rouge1': Score(precision=0.1595744680851064, recall=0.9090909090909091, fmeasure=0.27149321266968324), 'rouge2': Score(precision=0.11764705882352941, recall=0.6875, fmeasure=0.20091324200913244), 'rougeL': Score(precision=0.13829787234042554, recall=0.7878787878787878, fmeasure=0.23529411764705882)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.5050505050505051, recall=0.819672131147541, fmeasure=0.625), 'rouge2': Score(precision=0.29591836734693877, recall=0.48333333333333334, fmeasure=0.36708860759493667), 'rougeL': Score(precision=0.45454545454545453, recall=0.7377049180327869, fmeasure=0.5625)}, {'rouge1': Score(precision=0.4507042253521127, recall=0.5245901639344263, fmeasure=0.48484848484848486), 'rouge2': Score(precision=0.2857142857142857, recall=0.3333333333333333, fmeasure=0.30769230769230765), 'rougeL': Score(precision=0.38028169014084506, recall=0.4426229508196721, fmeasure=0.4090909090909091)}, {'rouge1': Score(precision=0.825, recall=0.825, fmeasure=0.825), 'rouge2': Score(precision=0.717948717948718, recall=0.717948717948718, fmeasure=0.717948717948718), 'rougeL': Score(precision=0.825, recall=0.825, fmeasure=0.825)}, {'rouge1': Score(precision=0.4105263157894737, recall=0.7959183673469388, fmeasure=0.5416666666666666), 'rouge2': Score(precision=0.32978723404255317, recall=0.6458333333333334, fmeasure=0.43661971830985913), 'rougeL': Score(precision=0.4, recall=0.7755102040816326, fmeasure=0.5277777777777778)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.96875, recall=0.9393939393939394, fmeasure=0.9538461538461539), 'rouge2': Score(precision=0.9032258064516129, recall=0.875, fmeasure=0.8888888888888888), 'rougeL': Score(precision=0.96875, recall=0.9393939393939394, fmeasure=0.9538461538461539)}, {'rouge1': Score(precision=0.38666666666666666, recall=0.7435897435897436, fmeasure=0.5087719298245614), 'rouge2': Score(precision=0.25675675675675674, recall=0.5, fmeasure=0.33928571428571425), 'rougeL': Score(precision=0.26666666666666666, recall=0.5128205128205128, fmeasure=0.3508771929824562)}, {'rouge1': Score(precision=0.5625, recall=0.8709677419354839, fmeasure=0.6835443037974683), 'rouge2': Score(precision=0.425531914893617, recall=0.6666666666666666, fmeasure=0.5194805194805194), 'rougeL': Score(precision=0.5208333333333334, recall=0.8064516129032258, fmeasure=0.6329113924050633)}, {'rouge1': Score(precision=0.15476190476190477, recall=0.30952380952380953, fmeasure=0.20634920634920637), 'rouge2': Score(precision=0.13253012048192772, recall=0.2682926829268293, fmeasure=0.17741935483870971), 'rougeL': Score(precision=0.15476190476190477, recall=0.30952380952380953, fmeasure=0.20634920634920637)}, {'rouge1': Score(precision=0.9, recall=0.8823529411764706, fmeasure=0.8910891089108911), 'rouge2': Score(precision=0.8163265306122449, recall=0.8, fmeasure=0.8080808080808082), 'rougeL': Score(precision=0.78, recall=0.7647058823529411, fmeasure=0.7722772277227723)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.41935483870967744, recall=0.8125, fmeasure=0.5531914893617021), 'rouge2': Score(precision=0.26229508196721313, recall=0.5161290322580645, fmeasure=0.34782608695652173), 'rougeL': Score(precision=0.2903225806451613, recall=0.5625, fmeasure=0.3829787234042554)}, {'rouge1': Score(precision=0.25263157894736843, recall=0.96, fmeasure=0.4), 'rouge2': Score(precision=0.17989417989417988, recall=0.6938775510204082, fmeasure=0.2857142857142857), 'rougeL': Score(precision=0.14736842105263157, recall=0.56, fmeasure=0.2333333333333333)}, {'rouge1': Score(precision=0.9473684210526315, recall=0.8181818181818182, fmeasure=0.8780487804878049), 'rouge2': Score(precision=0.8888888888888888, recall=0.7619047619047619, fmeasure=0.8205128205128205), 'rougeL': Score(precision=0.9473684210526315, recall=0.8181818181818182, fmeasure=0.8780487804878049)}, {'rouge1': Score(precision=0.5849056603773585, recall=0.6595744680851063, fmeasure=0.62), 'rouge2': Score(precision=0.4230769230769231, recall=0.4782608695652174, fmeasure=0.44897959183673475), 'rougeL': Score(precision=0.4528301886792453, recall=0.5106382978723404, fmeasure=0.48)}, {'rouge1': Score(precision=0.5747126436781609, recall=0.9433962264150944, fmeasure=0.7142857142857142), 'rouge2': Score(precision=0.5232558139534884, recall=0.8653846153846154, fmeasure=0.6521739130434783), 'rougeL': Score(precision=0.5402298850574713, recall=0.8867924528301887, fmeasure=0.6714285714285714)}, {'rouge1': Score(precision=0.42105263157894735, recall=0.8648648648648649, fmeasure=0.5663716814159292), 'rouge2': Score(precision=0.28, recall=0.5833333333333334, fmeasure=0.3783783783783784), 'rougeL': Score(precision=0.40789473684210525, recall=0.8378378378378378, fmeasure=0.5486725663716814)}, {'rouge1': Score(precision=0.3170731707317073, recall=0.4642857142857143, fmeasure=0.3768115942028986), 'rouge2': Score(precision=0.1728395061728395, recall=0.2545454545454545, fmeasure=0.20588235294117646), 'rougeL': Score(precision=0.24390243902439024, recall=0.35714285714285715, fmeasure=0.2898550724637681)}, {'rouge1': Score(precision=0.18235294117647058, recall=1.0, fmeasure=0.30845771144278605), 'rouge2': Score(precision=0.15384615384615385, recall=0.8666666666666667, fmeasure=0.2613065326633166), 'rougeL': Score(precision=0.16470588235294117, recall=0.9032258064516129, fmeasure=0.27860696517412936)}, {'rouge1': Score(precision=0.21052631578947367, recall=0.6274509803921569, fmeasure=0.31527093596059114), 'rouge2': Score(precision=0.059602649006622516, recall=0.18, fmeasure=0.08955223880597014), 'rougeL': Score(precision=0.11842105263157894, recall=0.35294117647058826, fmeasure=0.17733990147783252)}, {'rouge1': Score(precision=0.6956521739130435, recall=0.8888888888888888, fmeasure=0.7804878048780488), 'rouge2': Score(precision=0.6470588235294118, recall=0.8301886792452831, fmeasure=0.7272727272727273), 'rougeL': Score(precision=0.6956521739130435, recall=0.8888888888888888, fmeasure=0.7804878048780488)}, {'rouge1': Score(precision=0.3582089552238806, recall=0.96, fmeasure=0.5217391304347826), 'rouge2': Score(precision=0.3333333333333333, recall=0.9166666666666666, fmeasure=0.4888888888888888), 'rougeL': Score(precision=0.31343283582089554, recall=0.84, fmeasure=0.4565217391304348)}, {'rouge1': Score(precision=0.7422680412371134, recall=0.8470588235294118, fmeasure=0.7912087912087912), 'rouge2': Score(precision=0.5520833333333334, recall=0.6309523809523809, fmeasure=0.5888888888888888), 'rougeL': Score(precision=0.711340206185567, recall=0.8117647058823529, fmeasure=0.7582417582417583)}, {'rouge1': Score(precision=0.45901639344262296, recall=1.0, fmeasure=0.6292134831460674), 'rouge2': Score(precision=0.4380165289256198, recall=0.9636363636363636, fmeasure=0.6022727272727273), 'rougeL': Score(precision=0.45901639344262296, recall=1.0, fmeasure=0.6292134831460674)}, {'rouge1': Score(precision=0.26153846153846155, recall=0.7391304347826086, fmeasure=0.38636363636363635), 'rouge2': Score(precision=0.13178294573643412, recall=0.37777777777777777, fmeasure=0.19540229885057472), 'rougeL': Score(precision=0.23076923076923078, recall=0.6521739130434783, fmeasure=0.3409090909090909)}, {'rouge1': Score(precision=0.7142857142857143, recall=0.851063829787234, fmeasure=0.7766990291262136), 'rouge2': Score(precision=0.6, recall=0.717391304347826, fmeasure=0.6534653465346534), 'rougeL': Score(precision=0.7142857142857143, recall=0.851063829787234, fmeasure=0.7766990291262136)}, {'rouge1': Score(precision=0.6164383561643836, recall=0.9574468085106383, fmeasure=0.7500000000000001), 'rouge2': Score(precision=0.5555555555555556, recall=0.8695652173913043, fmeasure=0.6779661016949152), 'rougeL': Score(precision=0.4520547945205479, recall=0.7021276595744681, fmeasure=0.5499999999999999)}, {'rouge1': Score(precision=0.25, recall=0.8620689655172413, fmeasure=0.38759689922480617), 'rouge2': Score(precision=0.1717171717171717, recall=0.6071428571428571, fmeasure=0.26771653543307083), 'rougeL': Score(precision=0.15, recall=0.5172413793103449, fmeasure=0.23255813953488372)}, {'rouge1': Score(precision=0.2777777777777778, recall=0.9183673469387755, fmeasure=0.4265402843601896), 'rouge2': Score(precision=0.21739130434782608, recall=0.7291666666666666, fmeasure=0.33492822966507174), 'rougeL': Score(precision=0.2654320987654321, recall=0.8775510204081632, fmeasure=0.40758293838862564)}, {'rouge1': Score(precision=0.7833333333333333, recall=0.9306930693069307, fmeasure=0.8506787330316742), 'rouge2': Score(precision=0.7058823529411765, recall=0.84, fmeasure=0.767123287671233), 'rougeL': Score(precision=0.7166666666666667, recall=0.8514851485148515, fmeasure=0.7782805429864253)}, {'rouge1': Score(precision=0.5, recall=0.5757575757575758, fmeasure=0.5352112676056339), 'rouge2': Score(precision=0.2702702702702703, recall=0.3125, fmeasure=0.2898550724637681), 'rougeL': Score(precision=0.42105263157894735, recall=0.48484848484848486, fmeasure=0.4507042253521127)}, {'rouge1': Score(precision=0.42857142857142855, recall=0.6382978723404256, fmeasure=0.5128205128205128), 'rouge2': Score(precision=0.2318840579710145, recall=0.34782608695652173, fmeasure=0.2782608695652174), 'rougeL': Score(precision=0.3, recall=0.44680851063829785, fmeasure=0.35897435897435903)}, {'rouge1': Score(precision=0.32653061224489793, recall=0.6956521739130435, fmeasure=0.4444444444444445), 'rouge2': Score(precision=0.25, recall=0.5454545454545454, fmeasure=0.34285714285714286), 'rougeL': Score(precision=0.32653061224489793, recall=0.6956521739130435, fmeasure=0.4444444444444445)}, {'rouge1': Score(precision=0.8148148148148148, recall=0.9565217391304348, fmeasure=0.8800000000000001), 'rouge2': Score(precision=0.6981132075471698, recall=0.8222222222222222, fmeasure=0.7551020408163266), 'rougeL': Score(precision=0.7962962962962963, recall=0.9347826086956522, fmeasure=0.8599999999999999)}, {'rouge1': Score(precision=0.5726495726495726, recall=1.0, fmeasure=0.7282608695652174), 'rouge2': Score(precision=0.5689655172413793, recall=1.0, fmeasure=0.7252747252747253), 'rougeL': Score(precision=0.5726495726495726, recall=1.0, fmeasure=0.7282608695652174)}, {'rouge1': Score(precision=0.6576576576576577, recall=0.9012345679012346, fmeasure=0.7604166666666667), 'rouge2': Score(precision=0.5454545454545454, recall=0.75, fmeasure=0.631578947368421), 'rougeL': Score(precision=0.6216216216216216, recall=0.8518518518518519, fmeasure=0.7187499999999999)}, {'rouge1': Score(precision=0.6164383561643836, recall=0.6923076923076923, fmeasure=0.6521739130434783), 'rouge2': Score(precision=0.375, recall=0.421875, fmeasure=0.39705882352941174), 'rougeL': Score(precision=0.4383561643835616, recall=0.49230769230769234, fmeasure=0.463768115942029)}, {'rouge1': Score(precision=0.14935064935064934, recall=0.8846153846153846, fmeasure=0.25555555555555554), 'rouge2': Score(precision=0.12418300653594772, recall=0.76, fmeasure=0.21348314606741575), 'rougeL': Score(precision=0.14935064935064934, recall=0.8846153846153846, fmeasure=0.25555555555555554)}, {'rouge1': Score(precision=0.5, recall=0.6206896551724138, fmeasure=0.5538461538461539), 'rouge2': Score(precision=0.34285714285714286, recall=0.42857142857142855, fmeasure=0.38095238095238093), 'rougeL': Score(precision=0.3888888888888889, recall=0.4827586206896552, fmeasure=0.43076923076923074)}, {'rouge1': Score(precision=0.875, recall=0.875, fmeasure=0.875), 'rouge2': Score(precision=0.8461538461538461, recall=0.8461538461538461, fmeasure=0.8461538461538461), 'rougeL': Score(precision=0.875, recall=0.875, fmeasure=0.875)}, {'rouge1': Score(precision=0.8461538461538461, recall=0.9565217391304348, fmeasure=0.8979591836734695), 'rouge2': Score(precision=0.6862745098039216, recall=0.7777777777777778, fmeasure=0.7291666666666667), 'rougeL': Score(precision=0.7692307692307693, recall=0.8695652173913043, fmeasure=0.8163265306122449)}, {'rouge1': Score(precision=0.7, recall=0.8235294117647058, fmeasure=0.7567567567567567), 'rouge2': Score(precision=0.631578947368421, recall=0.75, fmeasure=0.6857142857142857), 'rougeL': Score(precision=0.7, recall=0.8235294117647058, fmeasure=0.7567567567567567)}, {'rouge1': Score(precision=0.3333333333333333, recall=1.0, fmeasure=0.5), 'rouge2': Score(precision=0.17647058823529413, recall=0.5454545454545454, fmeasure=0.26666666666666666), 'rougeL': Score(precision=0.2608695652173913, recall=0.782608695652174, fmeasure=0.391304347826087)}, {'rouge1': Score(precision=0.45689655172413796, recall=0.8983050847457628, fmeasure=0.6057142857142858), 'rouge2': Score(precision=0.3130434782608696, recall=0.6206896551724138, fmeasure=0.4161849710982659), 'rougeL': Score(precision=0.3620689655172414, recall=0.711864406779661, fmeasure=0.48)}, {'rouge1': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rouge2': Score(precision=1.0, recall=1.0, fmeasure=1.0), 'rougeL': Score(precision=1.0, recall=1.0, fmeasure=1.0)}, {'rouge1': Score(precision=0.8181818181818182, recall=0.9, fmeasure=0.8571428571428572), 'rouge2': Score(precision=0.7, recall=0.7777777777777778, fmeasure=0.7368421052631577), 'rougeL': Score(precision=0.8181818181818182, recall=0.9, fmeasure=0.8571428571428572)}, {'rouge1': Score(precision=0.5686274509803921, recall=0.8529411764705882, fmeasure=0.6823529411764706), 'rouge2': Score(precision=0.4, recall=0.6060606060606061, fmeasure=0.4819277108433735), 'rougeL': Score(precision=0.49019607843137253, recall=0.7352941176470589, fmeasure=0.588235294117647)}, {'rouge1': Score(precision=0.7272727272727273, recall=0.9411764705882353, fmeasure=0.8205128205128205), 'rouge2': Score(precision=0.5714285714285714, recall=0.75, fmeasure=0.6486486486486486), 'rougeL': Score(precision=0.6363636363636364, recall=0.8235294117647058, fmeasure=0.717948717948718)}, {'rouge1': Score(precision=0.975, recall=0.975, fmeasure=0.975), 'rouge2': Score(precision=0.9487179487179487, recall=0.9487179487179487, fmeasure=0.9487179487179487), 'rougeL': Score(precision=0.975, recall=0.975, fmeasure=0.975)}, {'rouge1': Score(precision=0.3192771084337349, recall=0.9137931034482759, fmeasure=0.4732142857142857), 'rouge2': Score(precision=0.23636363636363636, recall=0.6842105263157895, fmeasure=0.35135135135135137), 'rougeL': Score(precision=0.2891566265060241, recall=0.8275862068965517, fmeasure=0.42857142857142855)}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "#loads\n",
    "def load_data(csv_file):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    questions = data['Question'].tolist()\n",
    "    answers = data['Answer'].tolist()\n",
    "    return questions, answers\n",
    "\n",
    "#generate answer\n",
    "def generate_answers(questions, model, tokenizer):\n",
    "    generated_answers = []\n",
    "    for question in questions:\n",
    "        input_ids = tokenizer.encode(question, add_special_tokens=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        output_ids = model.generate(input_ids=input_ids.to(model.device), max_length=512, num_beams=5)\n",
    "        generated_answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        generated_answers.append(generated_answer)\n",
    "    return generated_answers\n",
    "\n",
    "#main\n",
    "csv_file = 'rougeeval.csv'\n",
    "save_path = 'textgenerator'\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained(save_path)\n",
    "\n",
    "questions, answers = load_data(csv_file)\n",
    "\n",
    "generated_answers = generate_answers(questions, model, tokenizer)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "#rouge score calculator\n",
    "rouge_scores = []\n",
    "for generated_answer, reference_answer in zip(generated_answers, answers):\n",
    "    score = scorer.score(generated_answer, reference_answer)\n",
    "    rouge_scores.append(score)\n",
    "\n",
    "print('Rouge scores:', rouge_scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: Swinburne Online offers a wide range of online education services, including degree programs, tutorials, eLearning Advisors, and dedicated online tutors in each of its Melbourne and overseas campuses. Learn more about our online degree programs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "#load model and tokenizer\n",
    "save_path = 'textgenerator'\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained(save_path)\n",
    "\n",
    "#use cpu if gpu is not available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "#generate answers\n",
    "while True:\n",
    "    question = input(\"Enter your question (or 'q' to quit): \")\n",
    "\n",
    "    if question.lower() == 'q':\n",
    "        break\n",
    "\n",
    "    #tokenise and encode question\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        question,\n",
    "        add_special_tokens=True,\n",
    "        padding='longest',\n",
    "        max_length=256,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    #generate answer\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            num_return_sequences=1,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    #decodes answer\n",
    "    generated_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Generated Answer:\", generated_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
